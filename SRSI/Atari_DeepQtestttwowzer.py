{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c0ff8d-7fd6-4f71-9ec8-8283454cddfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1c0ff8d-7fd6-4f71-9ec8-8283454cddfc",
    "outputId": "f4ed7434-fed4-47e2-bc93-de66d9c3d265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: ale_py in ./venv/lib/python3.12/site-packages (0.11.2)\n",
      "Requirement already satisfied: gymnasium in ./venv/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>1.20 in ./venv/lib/python3.12/site-packages (from ale_py) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./venv/lib/python3.12/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./venv/lib/python3.12/site-packages (from gymnasium) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./venv/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in ./venv/lib/python3.12/site-packages (from opencv-python) (2.2.6)\n",
      "Requirement already satisfied: pympler in ./venv/lib/python3.12/site-packages (1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "!pip3 install ale_py gymnasium\n",
    "!pip3 install opencv-python\n",
    "!pip3 install pympler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd0292b-3f06-49aa-81df-26dbceb4f2de",
   "metadata": {
    "id": "2cd0292b-3f06-49aa-81df-26dbceb4f2de"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import sys\n",
    "from pympler import asizeof\n",
    "import gc\n",
    "import time  # Add for timing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "from gymnasium.vector import SyncVectorEnv  # Correct import for gymnasium\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8480e4-8f64-4edb-9ebc-ccf0d59d1825",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "ff8480e4-8f64-4edb-9ebc-ccf0d59d1825",
    "outputId": "8c95c0d5-ce54-4b92-ec2f-8dca1f526902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vectorized environment with 8 parallel instances\n"
     ]
    }
   ],
   "source": [
    "gym.register_envs(ale_py)\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# Add parallel environment settings\n",
    "N_ENVS = 8  # Number of parallel environments\n",
    "N_STEPS = 100  # Collect 100 steps before training\n",
    "N_UPDATES = 4  # Number of training updates after collecting steps\n",
    "\n",
    "# Image cropping parameters\n",
    "CROP_TOP = 34\n",
    "CROP_BOTTOM = 16\n",
    "CROP_LEFT = 0\n",
    "CROP_RIGHT = 0\n",
    "RESIZE_WIDTH = 84\n",
    "RESIZE_HEIGHT = 84\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 0.1\n",
    "N_EPISODES = 100\n",
    "START_EPSILON = 0.7\n",
    "FINAL_EPSILON = 0.1\n",
    "EPSILON_DECAY = (START_EPSILON - FINAL_EPSILON) / (N_EPISODES * 1000)  # Decay per step\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "MEMORY_CAPACITY = 1000000\n",
    "MEMORY_FILL_SIZE = 50000\n",
    "MINIBATCH_SIZE = 8192\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "MAX_STEPS_PER_EPISODE = 5000\n",
    "REWARD_CLIP = True\n",
    "MODEL_FILE = \"dqn_pong\"\n",
    "LOAD_MODEL = \"dqn_pong_best.pth\"  # or False\n",
    "# =========================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create vectorized environment\n",
    "def make_env():\n",
    "    env = gym.make(\"ALE/Pong-v5\")\n",
    "    return env\n",
    "\n",
    "env_fns = [make_env for _ in range(N_ENVS)]\n",
    "vec_env = SyncVectorEnv(env_fns)\n",
    "print(f\"Created vectorized environment with {N_ENVS} parallel instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1583a4-ed90-4577-b3a7-9ee535065f3c",
   "metadata": {
    "id": "ce1583a4-ed90-4577-b3a7-9ee535065f3c"
   },
   "outputs": [],
   "source": [
    "# preprocessing image\n",
    "\n",
    "\n",
    "# preprocess image demonstration\n",
    "\"\"\"\n",
    "def preprocess_and_show_steps(obs):  # input: 210x160x3 RGB\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(18, 5))\n",
    "\n",
    "    # Step 1: Show original RGB image\n",
    "    axs[0].imshow(obs)\n",
    "    axs[0].set_title(\"Original RGB (210x160)\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Step 2: Convert to grayscale\n",
    "    greyscaled = np.dot(obs[..., :3], [0.299, 0.587, 0.114]).astype(np.uint8)\n",
    "    axs[1].imshow(greyscaled, cmap='gray')\n",
    "    axs[1].set_title(\"Grayscale\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Step 3: Crop image vertically (rows 95 to 195)\n",
    "    cropped = greyscaled[95:195, :]\n",
    "    axs[2].imshow(cropped, cmap='gray')\n",
    "    axs[2].set_title(\"Cropped (95:195)\")\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    # Step 4: Resize to 100x50 using nearest neighbor\n",
    "    resized = cv2.resize(cropped, dsize=(50, 25), interpolation=cv2.INTER_NEAREST)\n",
    "    axs[3].imshow(resized, cmap='gray')\n",
    "    axs[3].set_title(\"Resized to 100x50\")\n",
    "    axs[3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return resized\n",
    "\n",
    "# Run and visualize preprocessing steps\n",
    "obs, info = env.reset()\n",
    "processed = preprocess_and_show_steps(obs)\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(obs,\n",
    "               crop_top=CROP_TOP,\n",
    "               crop_bottom=CROP_BOTTOM,\n",
    "               crop_left=CROP_LEFT,\n",
    "               crop_right=CROP_RIGHT,\n",
    "               resize_width=RESIZE_WIDTH,\n",
    "               resize_height=RESIZE_HEIGHT):\n",
    "    # Convert to grayscale\n",
    "    if len(obs.shape) == 3 and obs.shape[2] == 3:\n",
    "        greyscaled = np.dot(obs[..., :3], [0.299, 0.587, 0.114])\n",
    "    else:\n",
    "        greyscaled = obs\n",
    "\n",
    "    # Get image dimensions\n",
    "    height, width = greyscaled.shape\n",
    "\n",
    "    # Calculate crop boundaries\n",
    "    top_bound = crop_top\n",
    "    bottom_bound = height - crop_bottom\n",
    "    left_bound = crop_left\n",
    "    right_bound = width - crop_right\n",
    "\n",
    "    # Perform cropping\n",
    "    cropped = greyscaled[top_bound:bottom_bound, left_bound:right_bound]\n",
    "\n",
    "    # Resize\n",
    "    resized = cv2.resize(cropped, (resize_width, resize_height),\n",
    "                         interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e43133-51f5-48e5-b925-c3118fa97bba",
   "metadata": {
    "id": "a2e43133-51f5-48e5-b925-c3118fa97bba"
   },
   "outputs": [],
   "source": [
    "# defining neural network\n",
    "# based on this architecture: https://arxiv.org/pdf/1312.5602\n",
    "# code written by Claude Sonnet 4\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions, in_channels=4):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Convolutional layers (standard DQN architecture)\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Dynamically calculate linear layer size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, RESIZE_HEIGHT, RESIZE_WIDTH)\n",
    "            dummy = F.relu(self.conv1(dummy))\n",
    "            dummy = F.relu(self.conv2(dummy))\n",
    "            dummy = F.relu(self.conv3(dummy))\n",
    "            self.linear_size = dummy.view(1, -1).size(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.linear_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# example input: batch of 32 preprocessed frames\n",
    "# batch_size = 32\n",
    "# input_tensor = torch.randn(batch_size, 4, 25, 50) # Corrected input shape\n",
    "\n",
    "# forward pass\n",
    "# model = DQN(env.action_space.n)\n",
    "# q_values = model(input_tensor)\n",
    "# print(f\"Input shape: {input_tensor.shape}\")\n",
    "# print(f\"Output Q-values shape: {q_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6eed2ab-d219-41f0-a80d-9b11b4608d48",
   "metadata": {
    "id": "a6eed2ab-d219-41f0-a80d-9b11b4608d48"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.state_shape = state_shape\n",
    "        # Pre-allocate contiguous arrays\n",
    "        self.states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.next_states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float16)  # Reduce precision\n",
    "        self.dones = np.zeros(capacity, dtype=np.bool_)\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.states[self.position] = state\n",
    "        self.next_states[self.position] = next_state\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.dones[self.position] = done\n",
    "\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        return (\n",
    "            self.states[indices],\n",
    "            self.actions[indices],\n",
    "            self.rewards[indices],\n",
    "            self.next_states[indices],\n",
    "            self.dones[indices]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def memory_usage_gb(self):\n",
    "        total_bytes = (self.states.nbytes + self.next_states.nbytes +\n",
    "                       self.actions.nbytes + self.rewards.nbytes +\n",
    "                       self.dones.nbytes)\n",
    "        return total_bytes / (1024 ** 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98a28015-3e2b-4656-ab05-1d8fc7de1b8d",
   "metadata": {
    "id": "98a28015-3e2b-4656-ab05-1d8fc7de1b8d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m replay_memory = ReplayMemory(MEMORY_CAPACITY, state_shape)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Initialize networks\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m online_net = DQN(\u001b[43menv\u001b[49m.action_space.n).to(device)\n\u001b[32m      7\u001b[39m target_net = DQN(env.action_space.n).to(device)\n\u001b[32m      8\u001b[39m target_net.load_state_dict(online_net.state_dict())\n",
      "\u001b[31mNameError\u001b[39m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize replay memory\n",
    "state_shape = (4, RESIZE_HEIGHT, RESIZE_WIDTH)\n",
    "replay_memory = ReplayMemory(MEMORY_CAPACITY, state_shape)\n",
    "\n",
    "# Initialize networks\n",
    "online_net = DQN(env.action_space.n).to(device)\n",
    "target_net = DQN(env.action_space.n).to(device)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# === Load saved weights to continue training ===\n",
    "if LOAD_MODEL:\n",
    "    online_net.load_state_dict(torch.load(LOAD_MODEL, map_location=device))\n",
    "    target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.RMSprop(\n",
    "    online_net.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    alpha=0.95,\n",
    "    momentum=0.95,\n",
    "    eps=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FS-G0nphfZUX",
   "metadata": {
    "id": "FS-G0nphfZUX"
   },
   "outputs": [],
   "source": [
    "class VectorAgent:\n",
    "    def __init__(self, num_envs, initial_epsilon, epsilon_decay, final_epsilon, discount_factor):\n",
    "        self.num_envs = num_envs\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_actions(self, states, model):\n",
    "        \"\"\"Get actions for all environments in a batch\"\"\"\n",
    "        # Convert states to tensor\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float32, device=device) / 255.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = model(states_tensor)\n",
    "\n",
    "        # Epsilon-greedy for each environment\n",
    "        actions = q_values.argmax(1).cpu().numpy()\n",
    "        rand_mask = np.random.rand(self.num_envs) < self.epsilon\n",
    "        actions[rand_mask] = np.array([vec_env.single_action_space.sample() for _ in range(self.num_envs)])[rand_mask]\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KJbwz_iTfp8H",
   "metadata": {
    "id": "KJbwz_iTfp8H"
   },
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "agent = VectorAgent(\n",
    "    num_envs=N_ENVS,\n",
    "    initial_epsilon=START_EPSILON,\n",
    "    epsilon_decay=EPSILON_DECAY,\n",
    "    final_epsilon=FINAL_EPSILON,\n",
    "    discount_factor=DISCOUNT_FACTOR,\n",
    ")\n",
    "\n",
    "# Training setup\n",
    "total_steps = 0\n",
    "episode_rewards = []\n",
    "episode_losses = []\n",
    "episode_steps = []\n",
    "best_reward = -float('inf')\n",
    "\n",
    "# Initialize frame buffers for each environment\n",
    "frame_buffers = [deque(maxlen=4) for _ in range(N_ENVS)]\n",
    "current_states = np.zeros((N_ENVS, 4, RESIZE_HEIGHT, RESIZE_WIDTH))\n",
    "\n",
    "def reset_env(i):\n",
    "    \"\"\"Reset a single environment\"\"\"\n",
    "    obs, _ = vec_env.reset(index=[i])\n",
    "    frame = preprocess(obs[0])\n",
    "    frame_buffers[i].clear()\n",
    "    for _ in range(4):\n",
    "        frame_buffers[i].append(frame)\n",
    "    return np.stack(frame_buffers[i], axis=0)\n",
    "\n",
    "# Initialize all environments\n",
    "print(\"Initializing parallel environments...\")\n",
    "obs, _ = vec_env.reset()\n",
    "for i in range(N_ENVS):\n",
    "    frame = preprocess(obs[i])\n",
    "    for _ in range(4):\n",
    "        frame_buffers[i].append(frame)\n",
    "    current_states[i] = np.stack(frame_buffers[i], axis=0)\n",
    "\n",
    "# Pre-fill replay memory\n",
    "print(\"Pre-filling replay memory...\")\n",
    "while len(replay_memory) < MEMORY_FILL_SIZE:\n",
    "    # Random actions for all environments\n",
    "    actions = np.array([vec_env.single_action_space.sample() for _ in range(N_ENVS)])\n",
    "\n",
    "# Inside the training loop:\n",
    "next_obs, rewards, dones, truncateds, infos = vec_env.step(actions)\n",
    "\n",
    "# Process each environment\n",
    "next_states = np.zeros_like(current_states)\n",
    "for i in range(N_ENVS):\n",
    "    next_frame = preprocess(next_obs[i])\n",
    "    frame_buffers[i].append(next_frame)\n",
    "    next_states[i] = np.stack(frame_buffers[i], axis=0)\n",
    "    \n",
    "    if REWARD_CLIP:\n",
    "        rewards[i] = np.clip(rewards[i], -1, 1)\n",
    "        \n",
    "    replay_memory.push(\n",
    "        current_states[i].copy(),\n",
    "        actions[i],\n",
    "        rewards[i],\n",
    "        next_states[i].copy(),\n",
    "        dones[i] or truncateds[i]\n",
    "    )\n",
    "    \n",
    "    if dones[i] or truncateds[i]:\n",
    "        next_states[i] = reset_env(i)\n",
    "        # Record episode completion\n",
    "        episode_rewards.append(episode_reward[i])\n",
    "        episode_steps.append(steps)\n",
    "        episode_reward[i] = 0\n",
    "\n",
    "\n",
    "print(f\"Replay memory filled with {len(replay_memory)} transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FDv2LXMUfZv5",
   "metadata": {
    "id": "FDv2LXMUfZv5"
   },
   "outputs": [],
   "source": [
    "# Reset environment and initialize frame stack\n",
    "obs, info = env.reset()\n",
    "frame = preprocess(obs)\n",
    "\n",
    "# Visualize the first preprocessed frame\n",
    "plt.imshow(frame, cmap='gray')\n",
    "plt.title(f\"Preprocessed Frame\\nSize: {frame.shape} | Crop: T:{CROP_TOP} B:{CROP_BOTTOM} L:{CROP_LEFT} R:{CROP_RIGHT}\")\n",
    "plt.show()\n",
    "\n",
    "frame_stack = deque(maxlen=4)\n",
    "for _ in range(4):\n",
    "    frame_stack.append(frame)\n",
    "state = np.stack(frame_stack, axis=0)\n",
    "\n",
    "# Pre-fill replay memory\n",
    "print(\"Pre-filling replay memory...\")\n",
    "for _ in tqdm(range(MEMORY_FILL_SIZE), desc=\"Pre-filling replay memory\"):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # Clip reward if enabled\n",
    "    if REWARD_CLIP:\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "\n",
    "    next_frame = preprocess(next_obs)\n",
    "    frame_stack.append(next_frame)\n",
    "    next_state = np.stack(frame_stack, axis=0)\n",
    "\n",
    "    replay_memory.push(state, action, reward, next_state, done)\n",
    "    state = next_state\n",
    "\n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "        frame = preprocess(obs)\n",
    "        frame_stack.clear()\n",
    "        for _ in range(4):\n",
    "            frame_stack.append(frame)\n",
    "        state = np.stack(frame_stack, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7df2a-991d-415c-b592-4a42af931bb9",
   "metadata": {
    "id": "65d7df2a-991d-415c-b592-4a42af931bb9"
   },
   "outputs": [],
   "source": [
    "# Training loop with parallel environments\n",
    "print(\"Starting training with parallel environments...\")\n",
    "total_updates = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in tqdm(range(N_EPISODES), desc=\"Training episodes\"):\n",
    "    steps = 0\n",
    "    episode_reward = np.zeros(N_ENVS)\n",
    "    step_losses = []\n",
    "\n",
    "    # Collect N_STEPS from all environments\n",
    "    for _ in range(N_STEPS):\n",
    "        # Get actions for all environments\n",
    "        actions = agent.get_actions(current_states, online_net)\n",
    "\n",
    "        # Step all environments\n",
    "        next_obs, rewards, dones, truncateds, _ = vec_env.step(actions)\n",
    "\n",
    "        # Process each environment\n",
    "        next_states = np.zeros_like(current_states)\n",
    "        for i in range(N_ENVS):\n",
    "            # Preprocess frame\n",
    "            next_frame = preprocess(next_obs[i])\n",
    "\n",
    "            # Update frame buffer\n",
    "            frame_buffers[i].append(next_frame)\n",
    "            next_states[i] = np.stack(frame_buffers[i], axis=0)\n",
    "\n",
    "            # Clip reward\n",
    "            if REWARD_CLIP:\n",
    "                rewards[i] = np.clip(rewards[i], -10, 10)\n",
    "\n",
    "            # Update episode reward\n",
    "            episode_reward[i] += rewards[i]\n",
    "\n",
    "            # Store transition\n",
    "            replay_memory.push(\n",
    "                current_states[i].copy(),\n",
    "                actions[i],\n",
    "                rewards[i],\n",
    "                next_states[i].copy(),\n",
    "                dones[i] or truncateds[i]\n",
    "            )\n",
    "\n",
    "            # Reset if done\n",
    "            if dones[i] or truncateds[i]:\n",
    "                next_states[i] = reset_env(i)\n",
    "                # Record completed episode\n",
    "                episode_rewards.append(episode_reward[i])\n",
    "                episode_steps.append(steps)\n",
    "                episode_reward[i] = 0\n",
    "\n",
    "        current_states = next_states\n",
    "        steps += 1\n",
    "        total_steps += N_ENVS\n",
    "\n",
    "    # Perform multiple training updates\n",
    "    for _ in range(N_UPDATES):\n",
    "        if len(replay_memory) > MINIBATCH_SIZE:\n",
    "            batch_data = replay_memory.sample(MINIBATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = batch_data\n",
    "\n",
    "            # Convert to tensors on GPU\n",
    "            states_tensor = torch.as_tensor(states, device=device, dtype=torch.float32) / 255.0\n",
    "            next_states_tensor = torch.as_tensor(next_states, device=device, dtype=torch.float32) / 255.0\n",
    "            actions_tensor = torch.as_tensor(actions, device=device, dtype=torch.long)\n",
    "            rewards_tensor = torch.as_tensor(rewards, device=device, dtype=torch.float32)\n",
    "            dones_tensor = torch.as_tensor(dones, device=device, dtype=torch.float32)\n",
    "\n",
    "            # Compute Q-values\n",
    "            current_q = online_net(states_tensor).gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(next_states_tensor).max(1)[0]\n",
    "                target_q = rewards_tensor + DISCOUNT_FACTOR * next_q * (1 - dones_tensor)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(current_q, target_q)\n",
    "            step_losses.append(loss.item())\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            for param in online_net.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_updates += 1\n",
    "\n",
    "    # Update target network periodically\n",
    "    if total_updates % (TARGET_UPDATE_FREQ // N_UPDATES) == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "    # Decay epsilon\n",
    "    for _ in range(N_STEPS * N_ENVS):\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "    # Record metrics\n",
    "    if step_losses:\n",
    "        avg_loss = sum(step_losses) / len(step_losses)\n",
    "        episode_losses.append(avg_loss)\n",
    "    else:\n",
    "        episode_losses.append(0)\n",
    "\n",
    "    # Save best model\n",
    "    current_avg_reward = np.mean(episode_rewards[-10:]) if episode_rewards else 0\n",
    "    if current_avg_reward > best_reward:\n",
    "        best_reward = current_avg_reward\n",
    "        torch.save(online_net.state_dict(), f\"{MODEL_FILE}_best.pth\")\n",
    "\n",
    "    # Logging\n",
    "    if episode % 10 == 0 and episode_rewards:\n",
    "        avg_reward = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards)\n",
    "        avg_steps = np.mean(episode_steps[-50:]) if len(episode_steps) >= 50 else np.mean(episode_steps)\n",
    "        mem_usage = replay_memory.memory_usage_gb()\n",
    "\n",
    "        # Calculate steps per second\n",
    "        elapsed = time.time() - start_time\n",
    "        steps_per_sec = total_steps / elapsed if elapsed > 0 else 0\n",
    "\n",
    "        print(f\"Episode {episode}: \"\n",
    "              f\"Avg Reward={avg_reward:.2f}, \"\n",
    "              f\"Avg Steps={avg_steps:.0f}, \"\n",
    "              f\"Epsilon={agent.epsilon:.4f}, \"\n",
    "              f\"Loss={episode_losses[-1]:.4f}, \"\n",
    "              f\"Steps={total_steps}, \"\n",
    "              f\"Speed={steps_per_sec:.1f} steps/s\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(online_net.state_dict(), f\"{MODEL_FILE}_final.pth\")\n",
    "print(\"Training complete. Models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0997e2-f9ee-4517-9786-8d63ac852139",
   "metadata": {
    "id": "ed0997e2-f9ee-4517-9786-8d63ac852139"
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(episode_losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "\n",
    "# Plot steps\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(episode_steps)\n",
    "plt.title(\"Episode Steps\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps Taken\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_results.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gKX-Po44dKsa",
   "metadata": {
    "id": "gKX-Po44dKsa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
