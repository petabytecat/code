{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1c0ff8d-7fd6-4f71-9ec8-8283454cddfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1c0ff8d-7fd6-4f71-9ec8-8283454cddfc",
    "outputId": "3e7271cb-14bc-4bde-d154-40931ddc023d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: ale_py in ./venv/lib/python3.12/site-packages (0.11.2)\n",
      "Requirement already satisfied: gymnasium in ./venv/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>1.20 in ./venv/lib/python3.12/site-packages (from ale_py) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./venv/lib/python3.12/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./venv/lib/python3.12/site-packages (from gymnasium) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./venv/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in ./venv/lib/python3.12/site-packages (from opencv-python) (2.2.6)\n",
      "Requirement already satisfied: pympler in ./venv/lib/python3.12/site-packages (1.1)\n",
      "Requirement already satisfied: logging in ./venv/lib/python3.12/site-packages (0.4.9.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "!pip3 install ale_py gymnasium\n",
    "!pip3 install opencv-python\n",
    "!pip3 install pympler\n",
    "!pip3 install logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cd0292b-3f06-49aa-81df-26dbceb4f2de",
   "metadata": {
    "id": "2cd0292b-3f06-49aa-81df-26dbceb4f2de"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import sys\n",
    "from pympler import asizeof\n",
    "import gc\n",
    "import time  # Add for timing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "from gymnasium.vector import SyncVectorEnv  # Correct import for gymnasium\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff8480e4-8f64-4edb-9ebc-ccf0d59d1825",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff8480e4-8f64-4edb-9ebc-ccf0d59d1825",
    "outputId": "18fba35d-9eee-4585-82d2-1f61fa6fe3e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Created vectorized environment with 8 parallel instances\n"
     ]
    }
   ],
   "source": [
    "gym.register_envs(ale_py)\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# Add parallel environment settings\n",
    "N_ENVS = 8  # Number of parallel environments\n",
    "N_STEPS = 100  # Collect 100 steps before training\n",
    "N_UPDATES = 4  # Number of training updates after collecting steps\n",
    "\n",
    "# Image cropping parameters\n",
    "CROP_TOP = 34\n",
    "CROP_BOTTOM = 16\n",
    "CROP_LEFT = 0\n",
    "CROP_RIGHT = 0\n",
    "RESIZE_WIDTH = 84\n",
    "RESIZE_HEIGHT = 84\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 0.1\n",
    "N_EPISODES = 100\n",
    "START_EPSILON = 0.7\n",
    "FINAL_EPSILON = 0.1\n",
    "EPSILON_DECAY = (START_EPSILON - FINAL_EPSILON) / (N_EPISODES * 1000)  # Decay per step\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "MEMORY_CAPACITY = 1000000\n",
    "MEMORY_FILL_SIZE = 50000\n",
    "MINIBATCH_SIZE = 8192\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "MAX_STEPS_PER_EPISODE = 5000\n",
    "REWARD_CLIP = True\n",
    "MODEL_FILE = \"dqn_pong\"\n",
    "LOAD_MODEL = \"dqn_pong_best.pth\"  # or False\n",
    "# =========================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create vectorized environment\n",
    "def make_env():\n",
    "    env = gym.make(\"ALE/Pong-v5\")\n",
    "    return env\n",
    "\n",
    "env_fns = [make_env for _ in range(N_ENVS)]\n",
    "vec_env = SyncVectorEnv(env_fns)\n",
    "print(f\"Created vectorized environment with {N_ENVS} parallel instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce1583a4-ed90-4577-b3a7-9ee535065f3c",
   "metadata": {
    "id": "ce1583a4-ed90-4577-b3a7-9ee535065f3c"
   },
   "outputs": [],
   "source": [
    "# preprocessing image\n",
    "\n",
    "\n",
    "# preprocess image demonstration\n",
    "\"\"\"\n",
    "def preprocess_and_show_steps(obs):  # input: 210x160x3 RGB\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(18, 5))\n",
    "\n",
    "    # Step 1: Show original RGB image\n",
    "    axs[0].imshow(obs)\n",
    "    axs[0].set_title(\"Original RGB (210x160)\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Step 2: Convert to grayscale\n",
    "    greyscaled = np.dot(obs[..., :3], [0.299, 0.587, 0.114]).astype(np.uint8)\n",
    "    axs[1].imshow(greyscaled, cmap='gray')\n",
    "    axs[1].set_title(\"Grayscale\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Step 3: Crop image vertically (rows 95 to 195)\n",
    "    cropped = greyscaled[95:195, :]\n",
    "    axs[2].imshow(cropped, cmap='gray')\n",
    "    axs[2].set_title(\"Cropped (95:195)\")\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    # Step 4: Resize to 100x50 using nearest neighbor\n",
    "    resized = cv2.resize(cropped, dsize=(50, 25), interpolation=cv2.INTER_NEAREST)\n",
    "    axs[3].imshow(resized, cmap='gray')\n",
    "    axs[3].set_title(\"Resized to 100x50\")\n",
    "    axs[3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return resized\n",
    "\n",
    "# Run and visualize preprocessing steps\n",
    "obs, info = env.reset()\n",
    "processed = preprocess_and_show_steps(obs)\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(obs,\n",
    "               crop_top=CROP_TOP,\n",
    "               crop_bottom=CROP_BOTTOM,\n",
    "               crop_left=CROP_LEFT,\n",
    "               crop_right=CROP_RIGHT,\n",
    "               resize_width=RESIZE_WIDTH,\n",
    "               resize_height=RESIZE_HEIGHT):\n",
    "    # Convert to grayscale\n",
    "    if len(obs.shape) == 3 and obs.shape[2] == 3:\n",
    "        greyscaled = np.dot(obs[..., :3], [0.299, 0.587, 0.114])\n",
    "    else:\n",
    "        greyscaled = obs\n",
    "\n",
    "    # Get image dimensions\n",
    "    height, width = greyscaled.shape\n",
    "\n",
    "    # Calculate crop boundaries\n",
    "    top_bound = crop_top\n",
    "    bottom_bound = height - crop_bottom\n",
    "    left_bound = crop_left\n",
    "    right_bound = width - crop_right\n",
    "\n",
    "    # Perform cropping\n",
    "    cropped = greyscaled[top_bound:bottom_bound, left_bound:right_bound]\n",
    "\n",
    "    # Resize\n",
    "    resized = cv2.resize(cropped, (resize_width, resize_height),\n",
    "                         interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2e43133-51f5-48e5-b925-c3118fa97bba",
   "metadata": {
    "id": "a2e43133-51f5-48e5-b925-c3118fa97bba"
   },
   "outputs": [],
   "source": [
    "# defining neural network\n",
    "# based on this architecture: https://arxiv.org/pdf/1312.5602\n",
    "# code written by Claude Sonnet 4\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions, in_channels=4):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Convolutional layers (standard DQN architecture)\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Dynamically calculate linear layer size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, RESIZE_HEIGHT, RESIZE_WIDTH)\n",
    "            dummy = F.relu(self.conv1(dummy))\n",
    "            dummy = F.relu(self.conv2(dummy))\n",
    "            dummy = F.relu(self.conv3(dummy))\n",
    "            self.linear_size = dummy.view(1, -1).size(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.linear_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# example input: batch of 32 preprocessed frames\n",
    "# batch_size = 32\n",
    "# input_tensor = torch.randn(batch_size, 4, 25, 50) # Corrected input shape\n",
    "\n",
    "# forward pass\n",
    "# model = DQN(env.action_space.n)\n",
    "# q_values = model(input_tensor)\n",
    "# print(f\"Input shape: {input_tensor.shape}\")\n",
    "# print(f\"Output Q-values shape: {q_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6eed2ab-d219-41f0-a80d-9b11b4608d48",
   "metadata": {
    "id": "a6eed2ab-d219-41f0-a80d-9b11b4608d48"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.state_shape = state_shape\n",
    "        # Pre-allocate contiguous arrays\n",
    "        self.states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.next_states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float16)  # Reduce precision\n",
    "        self.dones = np.zeros(capacity, dtype=np.bool_)\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.states[self.position] = state\n",
    "        self.next_states[self.position] = next_state\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.dones[self.position] = done\n",
    "\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        return (\n",
    "            self.states[indices],\n",
    "            self.actions[indices],\n",
    "            self.rewards[indices],\n",
    "            self.next_states[indices],\n",
    "            self.dones[indices]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def memory_usage_gb(self):\n",
    "        total_bytes = (self.states.nbytes + self.next_states.nbytes +\n",
    "                       self.actions.nbytes + self.rewards.nbytes +\n",
    "                       self.dones.nbytes)\n",
    "        return total_bytes / (1024 ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a28015-3e2b-4656-ab05-1d8fc7de1b8d",
   "metadata": {
    "id": "98a28015-3e2b-4656-ab05-1d8fc7de1b8d"
   },
   "outputs": [],
   "source": [
    "# Initialize replay memory\n",
    "state_shape = (4, RESIZE_HEIGHT, RESIZE_WIDTH)\n",
    "replay_memory = ReplayMemory(MEMORY_CAPACITY, state_shape)\n",
    "\n",
    "# Initialize networks\n",
    "online_net = DQN(gym.make(\"ALE/Pong-v5\").action_space.n).to(device)\n",
    "target_net = DQN(gym.make(\"ALE/Pong-v5\").action_space.n).to(device)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# === Load saved weights to continue training ===\n",
    "if LOAD_MODEL:\n",
    "    online_net.load_state_dict(torch.load(LOAD_MODEL, map_location=device))\n",
    "    target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.RMSprop(\n",
    "    online_net.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    alpha=0.95,\n",
    "    momentum=0.95,\n",
    "    eps=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "FS-G0nphfZUX",
   "metadata": {
    "id": "FS-G0nphfZUX"
   },
   "outputs": [],
   "source": [
    "class VectorAgent:\n",
    "    def __init__(self, num_envs, initial_epsilon, epsilon_decay, final_epsilon, discount_factor):\n",
    "        self.num_envs = num_envs\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_actions(self, states, model):\n",
    "        \"\"\"Get actions for all environments in a batch\"\"\"\n",
    "        # Convert states to tensor\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float32, device=device) / 255.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = model(states_tensor)\n",
    "\n",
    "        # Epsilon-greedy for each environment\n",
    "        actions = q_values.argmax(1).cpu().numpy()\n",
    "        rand_mask = np.random.rand(self.num_envs) < self.epsilon\n",
    "        actions[rand_mask] = np.array([vec_env.single_action_space.sample() for _ in range(self.num_envs)])[rand_mask]\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KJbwz_iTfp8H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "KJbwz_iTfp8H",
    "outputId": "a3d30b21-a593-43b5-a12f-2de1c5dc644b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating individual environments...\n",
      "Initializing parallel environments...\n",
      "Pre-filling replay memory to 50000 transitions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-filling:  32%|▎| 16008/50000 [00:13<00:27, 1231.36it/s, Memory=16008, Episod"
     ]
    }
   ],
   "source": [
    "# Initialize agent\n",
    "agent = VectorAgent(\n",
    "    num_envs=N_ENVS,\n",
    "    initial_epsilon=START_EPSILON,\n",
    "    epsilon_decay=EPSILON_DECAY,\n",
    "    final_epsilon=FINAL_EPSILON,\n",
    "    discount_factor=DISCOUNT_FACTOR,\n",
    ")\n",
    "\n",
    "# Training setup\n",
    "total_steps = 0\n",
    "episode_rewards = []\n",
    "episode_losses = []\n",
    "episode_steps = []\n",
    "best_reward = -float('inf')\n",
    "\n",
    "# Initialize frame buffers for each environment\n",
    "frame_buffers = [deque(maxlen=4) for _ in range(N_ENVS)]\n",
    "current_states = np.zeros((N_ENVS, 4, RESIZE_HEIGHT, RESIZE_WIDTH))\n",
    "\n",
    "# Create individual environments instead of vectorized env\n",
    "print(\"Creating individual environments...\")\n",
    "envs = [make_env() for _ in range(N_ENVS)]\n",
    "\n",
    "def reset_env(i):\n",
    "    \"\"\"Reset a single environment\"\"\"\n",
    "    obs, _ = envs[i].reset()\n",
    "    frame = preprocess(obs)\n",
    "    frame_buffers[i].clear()\n",
    "    for _ in range(4):\n",
    "        frame_buffers[i].append(frame)\n",
    "    return np.stack(frame_buffers[i], axis=0)\n",
    "\n",
    "# Initialize all environments\n",
    "print(\"Initializing parallel environments...\")\n",
    "for i in range(N_ENVS):\n",
    "    current_states[i] = reset_env(i)\n",
    "\n",
    "# Pre-fill replay memory \n",
    "print(f\"Pre-filling replay memory to {MEMORY_FILL_SIZE} transitions...\")\n",
    "pbar = tqdm(total=MEMORY_FILL_SIZE, desc=\"Pre-filling\")\n",
    "episodes_completed = 0\n",
    "\n",
    "# Track which environments are done\n",
    "done_flags = [False] * N_ENVS\n",
    "\n",
    "while len(replay_memory) < MEMORY_FILL_SIZE:\n",
    "    # Prepare actions\n",
    "    actions = []\n",
    "    for i in range(N_ENVS):\n",
    "        if done_flags[i]:\n",
    "            # Environment is done, we'll reset it below\n",
    "            actions.append(0)  # Dummy action, will be replaced\n",
    "        else:\n",
    "            # Get random action\n",
    "            actions.append(envs[i].action_space.sample())\n",
    "    \n",
    "    # Step all environments\n",
    "    next_obs_list = []\n",
    "    rewards_list = []\n",
    "    dones_list = []\n",
    "    truncateds_list = []\n",
    "    \n",
    "    for i in range(N_ENVS):\n",
    "        if done_flags[i]:\n",
    "            # Reset done environments\n",
    "            obs, _ = envs[i].reset()\n",
    "            frame = preprocess(obs)\n",
    "            frame_buffers[i].clear()\n",
    "            for _ in range(4):\n",
    "                frame_buffers[i].append(frame)\n",
    "            next_state = np.stack(frame_buffers[i], axis=0)\n",
    "            next_obs_list.append(obs)\n",
    "            rewards_list.append(0)\n",
    "            dones_list.append(False)\n",
    "            truncateds_list.append(False)\n",
    "            done_flags[i] = False\n",
    "        else:\n",
    "            # Step active environments\n",
    "            next_obs, reward, done, truncated, info = envs[i].step(actions[i])\n",
    "            next_obs_list.append(next_obs)\n",
    "            rewards_list.append(reward)\n",
    "            dones_list.append(done)\n",
    "            truncateds_list.append(truncated)\n",
    "    \n",
    "    # Process each environment\n",
    "    next_states = np.zeros_like(current_states)\n",
    "    for i in range(N_ENVS):\n",
    "        next_frame = preprocess(next_obs_list[i])\n",
    "        frame_buffers[i].append(next_frame)\n",
    "        next_states[i] = np.stack(frame_buffers[i], axis=0)\n",
    "        \n",
    "        if REWARD_CLIP:\n",
    "            rewards_list[i] = np.clip(rewards_list[i], -1, 1)\n",
    "        \n",
    "        # Only store transitions from active environments (not those that were just reset)\n",
    "        if not done_flags[i]:\n",
    "            replay_memory.push(\n",
    "                current_states[i].copy(),\n",
    "                actions[i],\n",
    "                rewards_list[i],\n",
    "                next_states[i].copy(),\n",
    "                dones_list[i] or truncateds_list[i]\n",
    "            )\n",
    "        \n",
    "        # Check if environment is done\n",
    "        if dones_list[i] or truncateds_list[i]:\n",
    "            done_flags[i] = True\n",
    "            episodes_completed += 1\n",
    "    \n",
    "    current_states = next_states\n",
    "    pbar.update(N_ENVS)\n",
    "    pbar.set_postfix({\"Memory\": len(replay_memory), \"Episodes\": episodes_completed})\n",
    "\n",
    "pbar.close()\n",
    "print(f\"Replay memory filled with {len(replay_memory)} transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d7df2a-991d-415c-b592-4a42af931bb9",
   "metadata": {
    "id": "65d7df2a-991d-415c-b592-4a42af931bb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with parallel environments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:   0%|                                | 0/100 [00:00<?, ?it/s]\n",
      "Episode 0 Steps:   0%|                                  | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Episode 0 Steps:   0%|  | 0/100 [00:00<?, ?it/s, Total Steps=1608, Avg Reward=0]\u001b[A\n",
      "Episode 0 Steps:   0%|  | 0/100 [00:00<?, ?it/s, Total Steps=1616, Avg Reward=0]\u001b[A\n",
      "Episode 0 Steps:   0%|  | 0/100 [00:00<?, ?it/s, Total Steps=1624, Avg Reward=0]\u001b[A\n",
      "Episode 0 Steps:   0%|  | 0/100 [00:00<?, ?it/s, Total Steps=1632, Avg Reward=0]\u001b[A\n",
      "Episode 0 Steps:   4%| | 4/100 [00:00<00:02, 37.76it/s, Total Steps=1632, Avg Re\u001b[A\n",
      "Episode 0 Steps:   4%| | 4/100 [00:00<00:02, 37.76it/s, Total Steps=1640, Avg Re\u001b[A\n",
      "Episode 0 Steps:   4%| | 4/100 [00:00<00:02, 37.76it/s, Total Steps=1648, Avg Re\u001b[A\n",
      "Episode 0 Steps:   4%| | 4/100 [00:00<00:02, 37.76it/s, Total Steps=1656, Avg Re\u001b[A\n",
      "Episode 0 Steps:   4%| | 4/100 [00:00<00:02, 37.76it/s, Total Steps=1664, Avg Re\u001b[A\n",
      "Episode 0 Steps:   4%| | 4/100 [00:00<00:02, 37.76it/s, Total Steps=1672, Avg Re\u001b[A\n",
      "Episode 0 Steps:   4%| | 4/100 [00:00<00:02, 37.76it/s, Total Steps=1680, Avg Re\u001b[A\n",
      "Episode 0 Steps:   4%| | 4/100 [00:00<00:02, 37.76it/s, Total Steps=1688, Avg Re\u001b[A\n",
      "Episode 0 Steps:   4%| | 4/100 [00:00<00:02, 37.76it/s, Total Steps=1696, Avg Re\u001b[A\n",
      "Episode 0 Steps:   4%| | 4/100 [00:00<00:02, 37.76it/s, Total Steps=1704, Avg Re\u001b[A\n",
      "Episode 0 Steps:  13%|▏| 13/100 [00:00<00:01, 67.07it/s, Total Steps=1704, Avg R\u001b[A\n",
      "Episode 0 Steps:  13%|▏| 13/100 [00:00<00:01, 67.07it/s, Total Steps=1712, Avg R\u001b[A\n",
      "Episode 0 Steps:  13%|▏| 13/100 [00:00<00:01, 67.07it/s, Total Steps=1720, Avg R\u001b[A\n",
      "Episode 0 Steps:  13%|▏| 13/100 [00:00<00:01, 67.07it/s, Total Steps=1728, Avg R\u001b[A\n",
      "Episode 0 Steps:  13%|▏| 13/100 [00:00<00:01, 67.07it/s, Total Steps=1736, Avg R\u001b[A\n",
      "Episode 0 Steps:  13%|▏| 13/100 [00:00<00:01, 67.07it/s, Total Steps=1744, Avg R\u001b[A\n",
      "Episode 0 Steps:  13%|▏| 13/100 [00:00<00:01, 67.07it/s, Total Steps=1752, Avg R\u001b[A\n",
      "Episode 0 Steps:  13%|▏| 13/100 [00:00<00:01, 67.07it/s, Total Steps=1760, Avg R\u001b[A\n",
      "Episode 0 Steps:  13%|▏| 13/100 [00:00<00:01, 67.07it/s, Total Steps=1768, Avg R\u001b[A\n",
      "Episode 0 Steps:  21%|▏| 21/100 [00:00<00:01, 65.56it/s, Total Steps=1768, Avg R\u001b[A\n",
      "Episode 0 Steps:  21%|▏| 21/100 [00:00<00:01, 65.56it/s, Total Steps=1776, Avg R\u001b[A\n",
      "Episode 0 Steps:  21%|▏| 21/100 [00:00<00:01, 65.56it/s, Total Steps=1784, Avg R\u001b[A\n",
      "Episode 0 Steps:  21%|▏| 21/100 [00:00<00:01, 65.56it/s, Total Steps=1792, Avg R\u001b[A\n",
      "Episode 0 Steps:  21%|▏| 21/100 [00:00<00:01, 65.56it/s, Total Steps=1800, Avg R\u001b[A\n",
      "Episode 0 Steps:  21%|▏| 21/100 [00:00<00:01, 65.56it/s, Total Steps=1808, Avg R\u001b[A\n",
      "Episode 0 Steps:  21%|▏| 21/100 [00:00<00:01, 65.56it/s, Total Steps=1816, Avg R\u001b[A\n",
      "Episode 0 Steps:  21%|▏| 21/100 [00:00<00:01, 65.56it/s, Total Steps=1824, Avg R\u001b[A\n",
      "Episode 0 Steps:  21%|▏| 21/100 [00:00<00:01, 65.56it/s, Total Steps=1832, Avg R\u001b[A\n",
      "Episode 0 Steps:  21%|▏| 21/100 [00:00<00:01, 65.56it/s, Total Steps=1840, Avg R\u001b[A\n",
      "Episode 0 Steps:  30%|▎| 30/100 [00:00<00:00, 72.62it/s, Total Steps=1840, Avg R\u001b[A\n",
      "Episode 0 Steps:  30%|▎| 30/100 [00:00<00:00, 72.62it/s, Total Steps=1848, Avg R\u001b[A\n",
      "Episode 0 Steps:  30%|▎| 30/100 [00:00<00:00, 72.62it/s, Total Steps=1856, Avg R\u001b[A\n",
      "Episode 0 Steps:  30%|▎| 30/100 [00:00<00:00, 72.62it/s, Total Steps=1864, Avg R\u001b[A\n",
      "Episode 0 Steps:  30%|▎| 30/100 [00:00<00:00, 72.62it/s, Total Steps=1872, Avg R\u001b[A\n",
      "Episode 0 Steps:  30%|▎| 30/100 [00:00<00:00, 72.62it/s, Total Steps=1880, Avg R\u001b[A\n",
      "Episode 0 Steps:  30%|▎| 30/100 [00:00<00:00, 72.62it/s, Total Steps=1888, Avg R\u001b[A\n",
      "Episode 0 Steps:  30%|▎| 30/100 [00:00<00:00, 72.62it/s, Total Steps=1896, Avg R\u001b[A\n",
      "Episode 0 Steps:  30%|▎| 30/100 [00:00<00:00, 72.62it/s, Total Steps=1904, Avg R\u001b[A\n",
      "Episode 0 Steps:  38%|▍| 38/100 [00:00<00:00, 71.44it/s, Total Steps=1904, Avg R\u001b[A\n",
      "Episode 0 Steps:  38%|▍| 38/100 [00:00<00:00, 71.44it/s, Total Steps=1912, Avg R\u001b[A\n",
      "Episode 0 Steps:  38%|▍| 38/100 [00:00<00:00, 71.44it/s, Total Steps=1920, Avg R\u001b[A\n",
      "Episode 0 Steps:  38%|▍| 38/100 [00:00<00:00, 71.44it/s, Total Steps=1928, Avg R\u001b[A\n",
      "Episode 0 Steps:  38%|▍| 38/100 [00:00<00:00, 71.44it/s, Total Steps=1936, Avg R\u001b[A\n",
      "Episode 0 Steps:  38%|▍| 38/100 [00:00<00:00, 71.44it/s, Total Steps=1944, Avg R\u001b[A\n",
      "Episode 0 Steps:  38%|▍| 38/100 [00:00<00:00, 71.44it/s, Total Steps=1952, Avg R\u001b[A\n",
      "Episode 0 Steps:  38%|▍| 38/100 [00:00<00:00, 71.44it/s, Total Steps=1960, Avg R\u001b[A\n",
      "Episode 0 Steps:  38%|▍| 38/100 [00:00<00:00, 71.44it/s, Total Steps=1968, Avg R\u001b[A\n",
      "Episode 0 Steps:  46%|▍| 46/100 [00:00<00:00, 56.44it/s, Total Steps=1968, Avg R\u001b[A\n",
      "Episode 0 Steps:  46%|▍| 46/100 [00:00<00:00, 56.44it/s, Total Steps=1976, Avg R\u001b[A\n",
      "Episode 0 Steps:  46%|▍| 46/100 [00:00<00:00, 56.44it/s, Total Steps=1984, Avg R\u001b[A\n",
      "Episode 0 Steps:  46%|▍| 46/100 [00:00<00:00, 56.44it/s, Total Steps=1992, Avg R\u001b[A\n",
      "Episode 0 Steps:  46%|▍| 46/100 [00:00<00:00, 56.44it/s, Total Steps=2000, Avg R\u001b[A\n",
      "Episode 0 Steps:  46%|▍| 46/100 [00:00<00:00, 56.44it/s, Total Steps=2008, Avg R\u001b[A\n",
      "Episode 0 Steps:  46%|▍| 46/100 [00:00<00:00, 56.44it/s, Total Steps=2016, Avg R\u001b[A\n",
      "Episode 0 Steps:  46%|▍| 46/100 [00:00<00:00, 56.44it/s, Total Steps=2024, Avg R\u001b[A\n",
      "Episode 0 Steps:  53%|▌| 53/100 [00:00<00:00, 53.22it/s, Total Steps=2024, Avg R\u001b[A\n",
      "Episode 0 Steps:  53%|▌| 53/100 [00:00<00:00, 53.22it/s, Total Steps=2032, Avg R\u001b[A\n",
      "Episode 0 Steps:  53%|▌| 53/100 [00:00<00:00, 53.22it/s, Total Steps=2040, Avg R\u001b[A\n",
      "Episode 0 Steps:  53%|▌| 53/100 [00:00<00:00, 53.22it/s, Total Steps=2048, Avg R\u001b[A\n",
      "Episode 0 Steps:  53%|▌| 53/100 [00:00<00:00, 53.22it/s, Total Steps=2056, Avg R\u001b[A\n",
      "Episode 0 Steps:  53%|▌| 53/100 [00:01<00:00, 53.22it/s, Total Steps=2064, Avg R\u001b[A\n",
      "Episode 0 Steps:  53%|▌| 53/100 [00:01<00:00, 53.22it/s, Total Steps=2072, Avg R\u001b[A\n",
      "Episode 0 Steps:  59%|▌| 59/100 [00:01<00:00, 47.76it/s, Total Steps=2072, Avg R\u001b[A\n",
      "Episode 0 Steps:  59%|▌| 59/100 [00:01<00:00, 47.76it/s, Total Steps=2080, Avg R\u001b[A\n",
      "Episode 0 Steps:  59%|▌| 59/100 [00:01<00:00, 47.76it/s, Total Steps=2088, Avg R\u001b[A\n",
      "Episode 0 Steps:  59%|▌| 59/100 [00:01<00:00, 47.76it/s, Total Steps=2096, Avg R\u001b[A\n",
      "Episode 0 Steps:  59%|▌| 59/100 [00:01<00:00, 47.76it/s, Total Steps=2104, Avg R\u001b[A\n",
      "Episode 0 Steps:  59%|▌| 59/100 [00:01<00:00, 47.76it/s, Total Steps=2112, Avg R\u001b[A\n",
      "Episode 0 Steps:  59%|▌| 59/100 [00:01<00:00, 47.76it/s, Total Steps=2120, Avg R\u001b[A\n",
      "Episode 0 Steps:  59%|▌| 59/100 [00:01<00:00, 47.76it/s, Total Steps=2128, Avg R\u001b[A\n",
      "Episode 0 Steps:  66%|▋| 66/100 [00:01<00:00, 52.02it/s, Total Steps=2128, Avg R\u001b[A\n",
      "Episode 0 Steps:  66%|▋| 66/100 [00:01<00:00, 52.02it/s, Total Steps=2136, Avg R\u001b[A\n",
      "Episode 0 Steps:  66%|▋| 66/100 [00:01<00:00, 52.02it/s, Total Steps=2144, Avg R\u001b[A\n",
      "Episode 0 Steps:  66%|▋| 66/100 [00:01<00:00, 52.02it/s, Total Steps=2152, Avg R\u001b[A\n",
      "Episode 0 Steps:  66%|▋| 66/100 [00:01<00:00, 52.02it/s, Total Steps=2160, Avg R\u001b[A\n",
      "Episode 0 Steps:  66%|▋| 66/100 [00:01<00:00, 52.02it/s, Total Steps=2168, Avg R\u001b[A\n",
      "Episode 0 Steps:  66%|▋| 66/100 [00:01<00:00, 52.02it/s, Total Steps=2176, Avg R\u001b[A\n",
      "Episode 0 Steps:  66%|▋| 66/100 [00:01<00:00, 52.02it/s, Total Steps=2184, Avg R\u001b[A\n",
      "Episode 0 Steps:  66%|▋| 66/100 [00:01<00:00, 52.02it/s, Total Steps=2192, Avg R\u001b[A\n",
      "Episode 0 Steps:  74%|▋| 74/100 [00:01<00:00, 55.08it/s, Total Steps=2192, Avg R\u001b[A\n",
      "Episode 0 Steps:  74%|▋| 74/100 [00:01<00:00, 55.08it/s, Total Steps=2200, Avg R\u001b[A\n",
      "Episode 0 Steps:  74%|▋| 74/100 [00:01<00:00, 55.08it/s, Total Steps=2208, Avg R\u001b[A\n",
      "Episode 0 Steps:  74%|▋| 74/100 [00:01<00:00, 55.08it/s, Total Steps=2216, Avg R\u001b[A\n",
      "Episode 0 Steps:  74%|▋| 74/100 [00:01<00:00, 55.08it/s, Total Steps=2224, Avg R\u001b[A\n",
      "Episode 0 Steps:  74%|▋| 74/100 [00:01<00:00, 55.08it/s, Total Steps=2232, Avg R\u001b[A\n",
      "Episode 0 Steps:  74%|▋| 74/100 [00:01<00:00, 55.08it/s, Total Steps=2240, Avg R\u001b[A\n",
      "Episode 0 Steps:  80%|▊| 80/100 [00:01<00:00, 55.03it/s, Total Steps=2240, Avg R\u001b[A\n",
      "Episode 0 Steps:  80%|▊| 80/100 [00:01<00:00, 55.03it/s, Total Steps=2248, Avg R\u001b[A\n",
      "Episode 0 Steps:  80%|▊| 80/100 [00:01<00:00, 55.03it/s, Total Steps=2256, Avg R\u001b[A\n",
      "Episode 0 Steps:  80%|▊| 80/100 [00:01<00:00, 55.03it/s, Total Steps=2264, Avg R\u001b[A\n",
      "Episode 0 Steps:  80%|▊| 80/100 [00:01<00:00, 55.03it/s, Total Steps=2272, Avg R\u001b[A\n",
      "Episode 0 Steps:  80%|▊| 80/100 [00:01<00:00, 55.03it/s, Total Steps=2280, Avg R\u001b[A\n",
      "Episode 0 Steps:  80%|▊| 80/100 [00:01<00:00, 55.03it/s, Total Steps=2288, Avg R\u001b[A\n",
      "Episode 0 Steps:  80%|▊| 80/100 [00:01<00:00, 55.03it/s, Total Steps=2296, Avg R\u001b[A\n",
      "Episode 0 Steps:  87%|▊| 87/100 [00:01<00:00, 56.58it/s, Total Steps=2296, Avg R\u001b[A\n",
      "Episode 0 Steps:  87%|▊| 87/100 [00:01<00:00, 56.58it/s, Total Steps=2304, Avg R\u001b[A\n",
      "Episode 0 Steps:  87%|▊| 87/100 [00:01<00:00, 56.58it/s, Total Steps=2312, Avg R\u001b[A\n",
      "Episode 0 Steps:  87%|▊| 87/100 [00:01<00:00, 56.58it/s, Total Steps=2320, Avg R\u001b[A\n",
      "Episode 0 Steps:  87%|▊| 87/100 [00:01<00:00, 56.58it/s, Total Steps=2328, Avg R\u001b[A\n",
      "Episode 0 Steps:  87%|▊| 87/100 [00:01<00:00, 56.58it/s, Total Steps=2336, Avg R\u001b[A\n",
      "Episode 0 Steps:  87%|▊| 87/100 [00:01<00:00, 56.58it/s, Total Steps=2344, Avg R\u001b[A\n",
      "Episode 0 Steps:  93%|▉| 93/100 [00:01<00:00, 55.69it/s, Total Steps=2344, Avg R\u001b[A\n",
      "Episode 0 Steps:  93%|▉| 93/100 [00:01<00:00, 55.69it/s, Total Steps=2352, Avg R\u001b[A\n",
      "Episode 0 Steps:  93%|▉| 93/100 [00:01<00:00, 55.69it/s, Total Steps=2360, Avg R\u001b[A\n",
      "Episode 0 Steps:  93%|▉| 93/100 [00:01<00:00, 55.69it/s, Total Steps=2368, Avg R\u001b[A\n",
      "Episode 0 Steps:  93%|▉| 93/100 [00:01<00:00, 55.69it/s, Total Steps=2376, Avg R\u001b[A\n",
      "Episode 0 Steps:  93%|▉| 93/100 [00:01<00:00, 55.69it/s, Total Steps=2384, Avg R\u001b[A\n",
      "Episode 0 Steps:  93%|▉| 93/100 [00:01<00:00, 55.69it/s, Total Steps=2392, Avg R\u001b[A\n",
      "Episode 0 Steps:  93%|▉| 93/100 [00:01<00:00, 55.69it/s, Total Steps=2400, Avg R\u001b[A\n",
      "Episode 0 Steps: 100%|█| 100/100 [00:01<00:00, 57.88it/s, Total Steps=2400, Avg \u001b[A\n",
      "                                                                                \u001b[A\n",
      "Training Updates:   0%|                                   | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Training Updates:   0%|                     | 0/4 [00:13<?, ?it/s, Loss=1.32e+7]\u001b[A\n",
      "Training Updates:  25%|███▎         | 1/4 [00:13<00:40, 13.41s/it, Loss=1.32e+7]\u001b[A\n",
      "Training Episodes:   0%|                                | 0/100 [00:24<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Compute target Q-values\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     next_q = \u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states_tensor\u001b[49m\u001b[43m)\u001b[49m.max(\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m    118\u001b[39m     target_q = rewards_tensor + DISCOUNT_FACTOR * next_q * (\u001b[32m1\u001b[39m - dones_tensor)\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/coding/0SRSI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/coding/0SRSI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mDQN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     26\u001b[39m     x = F.relu(\u001b[38;5;28mself\u001b[39m.conv1(x))\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     28\u001b[39m     x = F.relu(\u001b[38;5;28mself\u001b[39m.conv3(x))\n\u001b[32m     29\u001b[39m     x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/coding/0SRSI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/coding/0SRSI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/coding/0SRSI/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/coding/0SRSI/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Training loop with parallel environments - LOGGING VERSION\n",
    "logger.info(\"Starting training with parallel environments...\")\n",
    "total_updates = 0\n",
    "start_time = time.time()\n",
    "episode_rewards = []      # Track all completed episode rewards\n",
    "episode_steps_list = []   # Track all completed episode steps\n",
    "episode_losses = []       # Track losses per training update\n",
    "best_reward = -float('inf')\n",
    "\n",
    "# Keep the episode progress bar but remove nested bars\n",
    "episode_pbar = tqdm(range(N_EPISODES), desc=\"Training Episodes\")\n",
    "\n",
    "for episode in episode_pbar:\n",
    "    # Log episode start\n",
    "    logger.info(f\"Starting episode {episode}/{N_EPISODES}\")\n",
    "    \n",
    "    # Reset all environments at the start of each episode\n",
    "    obs, _ = vec_env.reset()\n",
    "    # Reinitialize frame buffers and current states\n",
    "    frame_buffers = [deque(maxlen=4) for _ in range(N_ENVS)]\n",
    "    current_states = np.zeros((N_ENVS, 4, RESIZE_HEIGHT, RESIZE_WIDTH))\n",
    "    for i in range(N_ENVS):\n",
    "        frame = preprocess(obs[i])\n",
    "        for _ in range(4):\n",
    "            frame_buffers[i].append(frame)\n",
    "        current_states[i] = np.stack(frame_buffers[i], axis=0)\n",
    "\n",
    "    # Per-environment tracking\n",
    "    episode_rewards_current = np.zeros(N_ENVS)   # Rewards for current episode in each env\n",
    "    episode_steps_current = np.zeros(N_ENVS)     # Steps for current episode in each env\n",
    "\n",
    "    # Collect N_STEPS from all environments - no nested progress bar\n",
    "    episode_start_time = time.time()\n",
    "    step_losses = []\n",
    "    \n",
    "    for step in range(N_STEPS):\n",
    "        # Get actions for all environments\n",
    "        actions = agent.get_actions(current_states, online_net)\n",
    "\n",
    "        # Step all environments\n",
    "        next_obs, rewards, dones, truncateds, infos = vec_env.step(actions)\n",
    "\n",
    "        # Process each environment\n",
    "        next_states = np.zeros_like(current_states)\n",
    "        for i in range(N_ENVS):\n",
    "            # Preprocess frame\n",
    "            next_frame = preprocess(next_obs[i])\n",
    "\n",
    "            # Update frame buffer\n",
    "            frame_buffers[i].append(next_frame)\n",
    "            next_states[i] = np.stack(frame_buffers[i], axis=0)\n",
    "\n",
    "            # Clip reward\n",
    "            if REWARD_CLIP:\n",
    "                rewards[i] = np.clip(rewards[i], -10, 10)\n",
    "\n",
    "            # Update tracking\n",
    "            episode_rewards_current[i] += rewards[i]\n",
    "            episode_steps_current[i] += 1\n",
    "\n",
    "            # Store transition\n",
    "            replay_memory.push(\n",
    "                current_states[i].copy(),\n",
    "                actions[i],\n",
    "                rewards[i],\n",
    "                next_states[i].copy(),\n",
    "                dones[i] or truncateds[i]\n",
    "            )\n",
    "\n",
    "            # Reset if done\n",
    "            if dones[i] or truncateds[i]:\n",
    "                # Record completed episode\n",
    "                episode_rewards.append(episode_rewards_current[i])\n",
    "                episode_steps_list.append(episode_steps_current[i])\n",
    "\n",
    "                # Log completed episode\n",
    "                logger.info(f\"Env {i} completed episode with reward: {episode_rewards_current[i]:.2f}\")\n",
    "\n",
    "                # Reset tracking\n",
    "                episode_rewards_current[i] = 0\n",
    "                episode_steps_current[i] = 0\n",
    "\n",
    "                # Reset environment\n",
    "                vec_env.reset_async(indices=[i])\n",
    "                reset_obs, _ = vec_env.reset_wait(indices=[i])\n",
    "                frame = preprocess(reset_obs[0])\n",
    "                frame_buffers[i].clear()\n",
    "                for _ in range(4):\n",
    "                    frame_buffers[i].append(frame)\n",
    "                next_states[i] = np.stack(frame_buffers[i], axis=0)\n",
    "\n",
    "        current_states = next_states\n",
    "        total_steps += N_ENVS\n",
    "\n",
    "        # Log every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:]) if episode_rewards else 0\n",
    "            logger.info(f\"Step {step}/{N_STEPS} - Total Steps: {total_steps} - Avg Reward: {avg_reward:.2f}\")\n",
    "\n",
    "    # Training updates - no nested progress bar\n",
    "    if len(replay_memory) > MINIBATCH_SIZE:\n",
    "        step_losses = []\n",
    "        for update in range(N_UPDATES):\n",
    "            batch_data = replay_memory.sample(MINIBATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = batch_data\n",
    "\n",
    "            # Convert to tensors\n",
    "            states_tensor = torch.as_tensor(states, device=device, dtype=torch.float32) / 255.0\n",
    "            next_states_tensor = torch.as_tensor(next_states, device=device, dtype=torch.float32) / 255.0\n",
    "            actions_tensor = torch.as_tensor(actions, device=device, dtype=torch.long)\n",
    "            rewards_tensor = torch.as_tensor(rewards, device=device, dtype=torch.float32)\n",
    "            dones_tensor = torch.as_tensor(dones, device=device, dtype=torch.float32)\n",
    "\n",
    "            # Compute Q-values\n",
    "            current_q = online_net(states_tensor).gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(next_states_tensor).max(1)[0]\n",
    "                target_q = rewards_tensor + DISCOUNT_FACTOR * next_q * (1 - dones_tensor)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(current_q, target_q)\n",
    "            step_losses.append(loss.item())\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            for param in online_net.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_updates += 1\n",
    "\n",
    "            # Log every 5 updates\n",
    "            if update % 5 == 0:\n",
    "                logger.info(f\"Update {update}/{N_UPDATES} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = sum(step_losses) / len(step_losses) if step_losses else 0\n",
    "        episode_losses.append(avg_loss)\n",
    "        logger.info(f\"Training updates completed - Avg Loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "        episode_losses.append(0)\n",
    "        logger.info(\"Skipping training updates (insufficient samples)\")\n",
    "\n",
    "    # Update target network\n",
    "    if total_updates % (TARGET_UPDATE_FREQ // N_UPDATES) == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "        logger.info(\"Target network updated\")\n",
    "\n",
    "    # Decay epsilon\n",
    "    for _ in range(N_STEPS * N_ENVS):\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_reward = np.mean(episode_rewards[-50:]) if episode_rewards else 0\n",
    "    avg_steps = np.mean(episode_steps_list[-50:]) if episode_steps_list else 0\n",
    "    elapsed = time.time() - start_time\n",
    "    steps_per_sec = total_steps / elapsed if elapsed > 0 else 0\n",
    "    episode_time = time.time() - episode_start_time\n",
    "\n",
    "    # Update episode progress bar\n",
    "    episode_pbar.set_postfix({\n",
    "        \"Avg Reward\": f\"{avg_reward:.2f}\",\n",
    "        \"Epsilon\": f\"{agent.epsilon:.4f}\",\n",
    "        \"Avg Loss\": f\"{episode_losses[-1]:.4f}\",\n",
    "        \"Speed\": f\"{steps_per_sec:.1f} steps/s\"\n",
    "    })\n",
    "\n",
    "    # Log episode completion\n",
    "    logger.info(f\"Episode {episode} completed in {episode_time:.2f} seconds\")\n",
    "    logger.info(f\"  Total Steps: {total_steps}\")\n",
    "    logger.info(f\"  Avg Reward (50): {avg_reward:.2f}\")\n",
    "    logger.info(f\"  Epsilon: {agent.epsilon:.4f}\")\n",
    "    logger.info(f\"  Avg Loss: {episode_losses[-1]:.4f}\")\n",
    "    logger.info(f\"  Speed: {steps_per_sec:.1f} steps/s\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_reward > best_reward:\n",
    "        best_reward = avg_reward\n",
    "        torch.save(online_net.state_dict(), f\"{MODEL_FILE}_best.pth\")\n",
    "        logger.info(f\"New best model saved with reward: {avg_reward:.2f}\")\n",
    "        episode_pbar.write(f\"New best model saved with reward: {avg_reward:.2f}\")\n",
    "\n",
    "# Final save and cleanup\n",
    "torch.save(online_net.state_dict(), f\"{MODEL_FILE}_final.pth\")\n",
    "episode_pbar.close()\n",
    "logger.info(\"Training complete. Models saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0997e2-f9ee-4517-9786-8d63ac852139",
   "metadata": {
    "id": "ed0997e2-f9ee-4517-9786-8d63ac852139"
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(episode_losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "\n",
    "# Plot steps\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(episode_steps)\n",
    "plt.title(\"Episode Steps\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps Taken\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_results.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gKX-Po44dKsa",
   "metadata": {
    "id": "gKX-Po44dKsa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
