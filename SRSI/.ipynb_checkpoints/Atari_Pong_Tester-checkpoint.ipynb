{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c0ff8d-7fd6-4f71-9ec8-8283454cddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in ./venv/lib/python3.12/site-packages (from opencv-python) (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gymnasium in ./venv/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./venv/lib/python3.12/site-packages (from gymnasium) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./venv/lib/python3.12/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./venv/lib/python3.12/site-packages (from gymnasium) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./venv/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ale-py in ./venv/lib/python3.12/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>1.20 in ./venv/lib/python3.12/site-packages (from ale-py) (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "!pip3 install opencv-python\n",
    "!pip3 install gymnasium\n",
    "!pip3 install ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32d22f66-0bea-4784-abab-5c1aef283b79",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OrderEnforcing' object has no attribute 'single_action_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Load trained model\u001b[39;00m\n\u001b[32m     60\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m model = \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     62\u001b[39m model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n\u001b[32m     63\u001b[39m model.eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mDQN.__init__\u001b[39m\u001b[34m(self, env)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env):\n\u001b[32m     38\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.network = torch.nn.Sequential(\n\u001b[32m     40\u001b[39m         torch.nn.Conv2d(\u001b[32m4\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m8\u001b[39m, stride=\u001b[32m4\u001b[39m),\n\u001b[32m     41\u001b[39m         torch.nn.ReLU(),\n\u001b[32m     42\u001b[39m         torch.nn.Conv2d(\u001b[32m32\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m4\u001b[39m, stride=\u001b[32m2\u001b[39m),\n\u001b[32m     43\u001b[39m         torch.nn.ReLU(),\n\u001b[32m     44\u001b[39m         torch.nn.Conv2d(\u001b[32m64\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m3\u001b[39m, stride=\u001b[32m1\u001b[39m),\n\u001b[32m     45\u001b[39m         torch.nn.ReLU(),\n\u001b[32m     46\u001b[39m         torch.nn.Flatten(),\n\u001b[32m     47\u001b[39m         torch.nn.Linear(\u001b[32m3136\u001b[39m, \u001b[32m512\u001b[39m),\n\u001b[32m     48\u001b[39m         torch.nn.ReLU(),\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         torch.nn.Linear(\u001b[32m512\u001b[39m, \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msingle_action_space\u001b[49m.n),\n\u001b[32m     50\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'OrderEnforcing' object has no attribute 'single_action_space'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from collections import deque\n",
    "import ale_py\n",
    "\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "MODEL_PATH = \"notebook_run123213213.cleanrl_model\"  # Path to your trained model\n",
    "NUM_EPISODES = 3                  # Number of games to play\n",
    "RENDER_DELAY = 5                 # ms between frames (0 for fastest)\n",
    "# =========================\n",
    "\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\", difficulty = 0)\n",
    "\n",
    "# Image preprocessing (must match training parameters)\n",
    "def preprocess(obs,\n",
    "               crop_top=34,\n",
    "               crop_bottom=16,\n",
    "               crop_left=0,\n",
    "               crop_right=0,\n",
    "               resize_width=84,\n",
    "               resize_height=84):\n",
    "    if len(obs.shape) == 3 and obs.shape[2] == 3:\n",
    "        greyscaled = np.dot(obs[..., :3], [0.299, 0.587, 0.114])\n",
    "    else:\n",
    "        greyscaled = obs\n",
    "    height, width = greyscaled.shape\n",
    "    cropped = greyscaled[crop_top:height-crop_bottom, crop_left:width-crop_right]\n",
    "    resized = cv2.resize(cropped, (resize_width, resize_height), interpolation=cv2.INTER_NEAREST)\n",
    "    return resized\n",
    "\n",
    "# Neural Network (must match training architecture)\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4, 32, 8, stride=4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 4, stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, 3, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(3136, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, env.single_action_space.n),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x / 255.0)\n",
    "\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\", difficulty = 0)\n",
    "\n",
    "# Load trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN(env.action_space.n).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model from {MODEL_PATH}\")\n",
    "print(f\"Playing {NUM_EPISODES} games...\")\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    obs, _ = env.reset()\n",
    "    frame = preprocess(obs)\n",
    "    frame_stack = deque([frame] * 4, maxlen=4)\n",
    "    state = np.stack(frame_stack, axis=0)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Prepare state tensor\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) / 255.0\n",
    "        \n",
    "        # Get Q-values and select best action\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "        action = q_values.argmax().item()\n",
    "        \n",
    "        # Take action\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        print(reward)\n",
    "        \n",
    "        # Update frame stack\n",
    "        next_frame = preprocess(next_obs)\n",
    "        frame_stack.append(next_frame)\n",
    "        next_state = np.stack(frame_stack, axis=0)\n",
    "        state = next_state\n",
    "        \n",
    "        # Add small delay for visualization\n",
    "        if RENDER_DELAY > 0:\n",
    "            cv2.waitKey(RENDER_DELAY)\n",
    "\n",
    "        # Show the preprocessed input frame (what the AI sees)\n",
    "        vis_frame = next_frame.astype(np.uint8)\n",
    "        cv2.imshow(\"AI Input (Preprocessed)\", vis_frame)\n",
    "        if cv2.waitKey(RENDER_DELAY) & 0xFF == ord('q'):\n",
    "            break  # Press 'q' to quit early\n",
    "\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Total Reward = {total_reward}\")\n",
    "\n",
    "env.close()\n",
    "print(\"Visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee6fa6-77a1-45ad-b07f-2642ee4bb03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debd7a2-ff90-4f3a-be72-9bbb240ec66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84380518-bdfa-4259-aa55-90a8093ed35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33213c2-7cc0-4391-a2f9-208806e0c29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1da255-f19f-4885-9210-4a944e4e4fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac405788-4949-4022-82a2-8b998711101a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b14ce-4b13-4f8e-bc6a-75c561abd6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
