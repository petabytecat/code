{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65f24384-139b-4045-81ff-a292819adfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Generator\n",
    "from typing import Any, NamedTuple, SupportsFloat\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import ale_py\n",
    "\n",
    "try:\n",
    "    # Check memory used by replay buffer when possible\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    psutil = None\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "\n",
    "    cv2.ocl.setUseOpenCL(False)\n",
    "except ImportError:\n",
    "    cv2 = None  # type: ignore[assignment]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da6946d4-9ffd-410d-b2a9-989b85f390fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cleanrl_utils.buffers import ReplayBuffer\n",
    "\n",
    "class ReplayBufferSamples(NamedTuple):\n",
    "    observations: th.Tensor\n",
    "    actions: th.Tensor\n",
    "    next_observations: th.Tensor\n",
    "    dones: th.Tensor\n",
    "    rewards: th.Tensor\n",
    "\n",
    "\n",
    "def get_device(device: th.device | str = \"auto\") -> th.device:\n",
    "    \"\"\"\n",
    "    Retrieve PyTorch device.\n",
    "    It checks that the requested device is available first.\n",
    "    For now, it supports only cpu and cuda.\n",
    "    By default, it tries to use the gpu.\n",
    "\n",
    "    :param device: One for 'auto', 'cuda', 'cpu'\n",
    "    :return: Supported Pytorch device\n",
    "    \"\"\"\n",
    "    # Cuda by default\n",
    "    if device == \"auto\":\n",
    "        device = \"cuda\"\n",
    "    # Force conversion to th.device\n",
    "    device = th.device(device)\n",
    "\n",
    "    # Cuda not available\n",
    "    if device.type == th.device(\"cuda\").type and not th.cuda.is_available():\n",
    "        return th.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def get_obs_shape(\n",
    "    observation_space: spaces.Space,\n",
    ") -> tuple[int, ...] | dict[str, tuple[int, ...]]:\n",
    "    \"\"\"\n",
    "    Get the shape of the observation (useful for the buffers).\n",
    "\n",
    "    :param observation_space:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(observation_space, spaces.Box):\n",
    "        return observation_space.shape\n",
    "    elif isinstance(observation_space, spaces.Discrete):\n",
    "        # Observation is an int\n",
    "        return (1,)\n",
    "    elif isinstance(observation_space, spaces.MultiDiscrete):\n",
    "        # Number of discrete features\n",
    "        return (int(len(observation_space.nvec)),)\n",
    "    elif isinstance(observation_space, spaces.MultiBinary):\n",
    "        # Number of binary features\n",
    "        return observation_space.shape\n",
    "    elif isinstance(observation_space, spaces.Dict):\n",
    "        return {key: get_obs_shape(subspace) for (key, subspace) in observation_space.spaces.items()}  # type: ignore[misc]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{observation_space} observation space is not supported\")\n",
    "\n",
    "def get_action_dim(action_space: spaces.Space) -> int:\n",
    "    \"\"\"\n",
    "    Get the dimension of the action space.\n",
    "\n",
    "    :param action_space:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(action_space, spaces.Box):\n",
    "        return int(np.prod(action_space.shape))\n",
    "    elif isinstance(action_space, spaces.Discrete):\n",
    "        # Action is an int\n",
    "        return 1\n",
    "    elif isinstance(action_space, spaces.MultiDiscrete):\n",
    "        # Number of discrete actions\n",
    "        return int(len(action_space.nvec))\n",
    "    elif isinstance(action_space, spaces.MultiBinary):\n",
    "        # Number of binary actions\n",
    "        assert isinstance(\n",
    "            action_space.n, int\n",
    "        ), f\"Multi-dimensional MultiBinary({action_space.n}) action space is not supported. You can flatten it instead.\"\n",
    "        return int(action_space.n)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{action_space} action space is not supported\")\n",
    "\n",
    "\n",
    "\n",
    "class BaseBuffer(ABC):\n",
    "    \"\"\"\n",
    "    Base class that represent a buffer (rollout or replay)\n",
    "\n",
    "    :param buffer_size: Max number of element in the buffer\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param device: PyTorch device\n",
    "        to which the values will be converted\n",
    "    :param n_envs: Number of parallel environments\n",
    "    \"\"\"\n",
    "\n",
    "    observation_space: spaces.Space\n",
    "    obs_shape: tuple[int, ...]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        device: torch.device | str = \"auto\",\n",
    "        n_envs: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.buffer_size = buffer_size\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.obs_shape = get_obs_shape(observation_space)  # type: ignore[assignment]\n",
    "\n",
    "        self.action_dim = get_action_dim(action_space)\n",
    "        self.pos = 0\n",
    "        self.full = False\n",
    "        self.device = get_device(device)\n",
    "        self.n_envs = n_envs\n",
    "\n",
    "    @staticmethod\n",
    "    def swap_and_flatten(arr: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Swap and then flatten axes 0 (buffer_size) and 1 (n_envs)\n",
    "        to convert shape from [n_steps, n_envs, ...] (when ... is the shape of the features)\n",
    "        to [n_steps * n_envs, ...] (which maintain the order)\n",
    "\n",
    "        :param arr:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        shape = arr.shape\n",
    "        if len(shape) < 3:\n",
    "            shape = (*shape, 1)\n",
    "        return arr.swapaxes(0, 1).reshape(shape[0] * shape[1], *shape[2:])\n",
    "\n",
    "    def size(self) -> int:\n",
    "        \"\"\"\n",
    "        :return: The current size of the buffer\n",
    "        \"\"\"\n",
    "        if self.full:\n",
    "            return self.buffer_size\n",
    "        return self.pos\n",
    "\n",
    "    def add(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Add elements to the buffer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def extend(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Add a new batch of transitions to the buffer\n",
    "        \"\"\"\n",
    "        # Do a for loop along the batch axis\n",
    "        for data in zip(*args):\n",
    "            self.add(*data)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the buffer.\n",
    "        \"\"\"\n",
    "        self.pos = 0\n",
    "        self.full = False\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        :param batch_size: Number of element to sample\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        upper_bound = self.buffer_size if self.full else self.pos\n",
    "        batch_inds = np.random.randint(0, upper_bound, size=batch_size)\n",
    "        return self._get_samples(batch_inds)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_samples(self, batch_inds: np.ndarray) -> ReplayBufferSamples | RolloutBufferSamples:\n",
    "        \"\"\"\n",
    "        :param batch_inds:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def to_torch(self, array: np.ndarray, copy: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert a numpy array to a PyTorch tensor.\n",
    "        Note: it copies the data by default\n",
    "\n",
    "        :param array:\n",
    "        :param copy: Whether to copy or not the data (may be useful to avoid changing things\n",
    "            by reference). This argument is inoperative if the device is not the CPU.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if copy:\n",
    "            return torch.tensor(array, device=self.device)\n",
    "        return torch.as_tensor(array, device=self.device)\n",
    "\n",
    "\n",
    "class ReplayBuffer(BaseBuffer):\n",
    "    \"\"\"\n",
    "    Replay buffer used in off-policy algorithms like SAC/TD3.\n",
    "\n",
    "    :param buffer_size: Max number of element in the buffer\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param device: PyTorch device\n",
    "    :param n_envs: Number of parallel environments\n",
    "    :param optimize_memory_usage: Enable a memory efficient variant\n",
    "        of the replay buffer which reduces by almost a factor two the memory used,\n",
    "        at a cost of more complexity.\n",
    "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
    "        and https://github.com/DLR-RM/stable-baselines3/pull/28#issuecomment-637559274\n",
    "        Cannot be used in combination with handle_timeout_termination.\n",
    "    :param handle_timeout_termination: Handle timeout termination (due to timelimit)\n",
    "        separately and treat the task as infinite horizon task.\n",
    "        https://github.com/DLR-RM/stable-baselines3/issues/284\n",
    "    \"\"\"\n",
    "\n",
    "    observations: np.ndarray\n",
    "    next_observations: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    dones: np.ndarray\n",
    "    timeouts: np.ndarray\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        device: torch.device | str = \"auto\",\n",
    "        n_envs: int = 1,\n",
    "        optimize_memory_usage: bool = False,\n",
    "        handle_timeout_termination: bool = True,\n",
    "    ):\n",
    "        super().__init__(buffer_size, observation_space, action_space, device, n_envs=n_envs)\n",
    "\n",
    "        # Adjust buffer size\n",
    "        self.buffer_size = max(buffer_size // n_envs, 1)\n",
    "\n",
    "        # Check that the replay buffer can fit into the memory\n",
    "        if psutil is not None:\n",
    "            mem_available = psutil.virtual_memory().available\n",
    "\n",
    "        # there is a bug if both optimize_memory_usage and handle_timeout_termination are true\n",
    "        # see https://github.com/DLR-RM/stable-baselines3/issues/934\n",
    "        if optimize_memory_usage and handle_timeout_termination:\n",
    "            raise ValueError(\n",
    "                \"ReplayBuffer does not support optimize_memory_usage = True \"\n",
    "                \"and handle_timeout_termination = True simultaneously.\"\n",
    "            )\n",
    "        self.optimize_memory_usage = optimize_memory_usage\n",
    "\n",
    "        self.observations = np.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=observation_space.dtype)\n",
    "\n",
    "        if not optimize_memory_usage:\n",
    "            # When optimizing memory, `observations` contains also the next observation\n",
    "            self.next_observations = np.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=observation_space.dtype)\n",
    "\n",
    "        self.actions = np.zeros(\n",
    "            (self.buffer_size, self.n_envs, self.action_dim), dtype=self._maybe_cast_dtype(action_space.dtype)\n",
    "        )\n",
    "\n",
    "        self.rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "        self.dones = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "        # Handle timeouts termination properly if needed\n",
    "        # see https://github.com/DLR-RM/stable-baselines3/issues/284\n",
    "        self.handle_timeout_termination = handle_timeout_termination\n",
    "        self.timeouts = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
    "\n",
    "        if psutil is not None:\n",
    "            total_memory_usage: float = (\n",
    "                self.observations.nbytes + self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
    "            )\n",
    "\n",
    "            if not optimize_memory_usage:\n",
    "                total_memory_usage += self.next_observations.nbytes\n",
    "\n",
    "            if total_memory_usage > mem_available:\n",
    "                # Convert to GB\n",
    "                total_memory_usage /= 1e9\n",
    "                mem_available /= 1e9\n",
    "                warnings.warn(\n",
    "                    \"This system does not have apparently enough memory to store the complete \"\n",
    "                    f\"replay buffer {total_memory_usage:.2f}GB > {mem_available:.2f}GB\"\n",
    "                )\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        next_obs: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: np.ndarray,\n",
    "        done: np.ndarray,\n",
    "        infos: list[dict[str, Any]],\n",
    "    ) -> None:\n",
    "        # Reshape needed when using multiple envs with discrete observations\n",
    "        # as numpy cannot broadcast (n_discrete,) to (n_discrete, 1)\n",
    "        if isinstance(self.observation_space, spaces.Discrete):\n",
    "            obs = obs.reshape((self.n_envs, *self.obs_shape))\n",
    "            next_obs = next_obs.reshape((self.n_envs, *self.obs_shape))\n",
    "\n",
    "        # Reshape to handle multi-dim and discrete action spaces, see GH #970 #1392\n",
    "        action = action.reshape((self.n_envs, self.action_dim))\n",
    "\n",
    "        # Copy to avoid modification by reference\n",
    "        self.observations[self.pos] = np.array(obs)\n",
    "\n",
    "        if self.optimize_memory_usage:\n",
    "            self.observations[(self.pos + 1) % self.buffer_size] = np.array(next_obs)\n",
    "        else:\n",
    "            self.next_observations[self.pos] = np.array(next_obs)\n",
    "\n",
    "        self.actions[self.pos] = np.array(action)\n",
    "        self.rewards[self.pos] = np.array(reward)\n",
    "        self.dones[self.pos] = np.array(done)\n",
    "\n",
    "        if self.handle_timeout_termination:\n",
    "            self.timeouts[self.pos] = np.array([info.get(\"TimeLimit.truncated\", False) for info in infos])\n",
    "\n",
    "        self.pos += 1\n",
    "        if self.pos == self.buffer_size:\n",
    "            self.full = True\n",
    "            self.pos = 0\n",
    "\n",
    "    def sample(self, batch_size: int) -> ReplayBufferSamples:\n",
    "        \"\"\"\n",
    "        Sample elements from the replay buffer.\n",
    "        Custom sampling when using memory efficient variant,\n",
    "        as we should not sample the element with index `self.pos`\n",
    "        See https://github.com/DLR-RM/stable-baselines3/pull/28#issuecomment-637559274\n",
    "\n",
    "        :param batch_size: Number of element to sample\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if not self.optimize_memory_usage:\n",
    "            return super().sample(batch_size=batch_size)\n",
    "        # Do not sample the element with index `self.pos` as the transitions is invalid\n",
    "        # (we use only one array to store `obs` and `next_obs`)\n",
    "        if self.full:\n",
    "            batch_inds = (np.random.randint(1, self.buffer_size, size=batch_size) + self.pos) % self.buffer_size\n",
    "        else:\n",
    "            batch_inds = np.random.randint(0, self.pos, size=batch_size)\n",
    "        return self._get_samples(batch_inds)\n",
    "\n",
    "    def _get_samples(self, batch_inds: np.ndarray) -> ReplayBufferSamples:\n",
    "        # Sample randomly the env idx\n",
    "        env_indices = np.random.randint(0, high=self.n_envs, size=(len(batch_inds),))\n",
    "\n",
    "        if self.optimize_memory_usage:\n",
    "            next_obs = self.observations[(batch_inds + 1) % self.buffer_size, env_indices, :]\n",
    "        else:\n",
    "            next_obs = self.next_observations[batch_inds, env_indices, :]\n",
    "\n",
    "        data = (\n",
    "            self.observations[batch_inds, env_indices, :],\n",
    "            self.actions[batch_inds, env_indices, :],\n",
    "            next_obs,\n",
    "            # Only use dones that are not due to timeouts\n",
    "            # deactivated by default (timeouts is initialized as an array of False)\n",
    "            (self.dones[batch_inds, env_indices] * (1 - self.timeouts[batch_inds, env_indices])).reshape(-1, 1),\n",
    "            self.rewards[batch_inds, env_indices].reshape(-1, 1),\n",
    "        )\n",
    "        return ReplayBufferSamples(*tuple(map(self.to_torch, data)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _maybe_cast_dtype(dtype: np.typing.DTypeLike) -> np.typing.DTypeLike:\n",
    "        \"\"\"\n",
    "        Cast `np.float64` action datatype to `np.float32`,\n",
    "        keep the others dtype unchanged.\n",
    "        See GH#1572 for more information.\n",
    "\n",
    "        :param dtype: The original action space dtype\n",
    "        :return: ``np.float32`` if the dtype was float64,\n",
    "            the original dtype otherwise.\n",
    "        \"\"\"\n",
    "        if dtype == np.float64:\n",
    "            return np.float32\n",
    "        return dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f7bf75d-8cb2-4b94-bcb3-bc2d8738cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Clip the reward to {+1, 0, -1} by its sign.\n",
    "\n",
    "    :param env: Environment to wrap\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reward(self, reward: SupportsFloat) -> float:\n",
    "        \"\"\"\n",
    "        Bin reward to {+1, 0, -1} by its sign.\n",
    "\n",
    "        :param reward:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.sign(float(reward))\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n",
    "    \"\"\"\n",
    "    Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "    Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "\n",
    "    :param env: Environment to wrap\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        super().__init__(env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action: int):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.was_real_done = terminated or truncated\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()  # type: ignore[attr-defined]\n",
    "        if 0 < lives < self.lives:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            terminated = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Calls the Gym environment reset, only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "\n",
    "        :param kwargs: Extra keywords passed to env.reset() call\n",
    "        :return: the first observation of the environment\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, terminated, truncated, info = self.env.step(0)\n",
    "\n",
    "            # The no-op step can lead to a game over, so we need to check it again\n",
    "            # to see if we should reset the environment and avoid the\n",
    "            # monitor.py `RuntimeError: Tried to step environment that needs reset`\n",
    "            if terminated or truncated:\n",
    "                obs, info = self.env.reset(**kwargs)\n",
    "        self.lives = self.env.unwrapped.ale.lives()  # type: ignore[attr-defined]\n",
    "        return obs, info\n",
    "\n",
    "class FireResetEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n",
    "    \"\"\"\n",
    "    Take action on reset for environments that are fixed until firing.\n",
    "\n",
    "    :param env: Environment to wrap\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        super().__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"  # type: ignore[attr-defined]\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3  # type: ignore[attr-defined]\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, terminated, truncated, _ = self.env.step(1)\n",
    "        if terminated or truncated:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, terminated, truncated, _ = self.env.step(2)\n",
    "        if terminated or truncated:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs, {}\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n",
    "    \"\"\"\n",
    "    Return only every ``skip``-th frame (frameskipping)\n",
    "    and return the max between the two last frames.\n",
    "\n",
    "    :param env: Environment to wrap\n",
    "    :param skip: Number of ``skip``-th frame\n",
    "        The same action will be taken ``skip`` times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, skip: int = 4) -> None:\n",
    "        super().__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        assert env.observation_space.dtype is not None, \"No dtype specified for the observation space\"\n",
    "        assert env.observation_space.shape is not None, \"No shape defined for the observation space\"\n",
    "        self._obs_buffer = np.zeros((2, *env.observation_space.shape), dtype=env.observation_space.dtype)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action: int):\n",
    "        \"\"\"\n",
    "        Step the environment with the given action\n",
    "        Repeat action, sum reward, and max over last observations.\n",
    "\n",
    "        :param action: the action\n",
    "        :return: observation, reward, terminated, truncated, information\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        terminated = truncated = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += float(reward)\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, terminated, truncated, info\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n",
    "    \"\"\"\n",
    "    Sample initial states by taking random number of no-ops on reset.\n",
    "    No-op is assumed to be action 0.\n",
    "\n",
    "    :param env: Environment to wrap\n",
    "    :param noop_max: Maximum value of no-ops to run\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, noop_max: int = 30) -> None:\n",
    "        super().__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"  # type: ignore[attr-defined]\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        obs = np.zeros(0)\n",
    "        info: dict = {}\n",
    "        for _ in range(noops):\n",
    "            obs, _, terminated, truncated, info = self.env.step(self.noop_action)\n",
    "            if terminated or truncated:\n",
    "                obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ddffb28-7405-414a-b676-9db48c003104",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    script_name: str = os.path.basename(__file__)[: -len(\".py\")]\n",
    "except NameError:\n",
    "    script_name: str = \"notebook_run\"\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = script_name\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = False\n",
    "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
    "    upload_model: bool = False\n",
    "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
    "    hf_entity: str = \"\"\n",
    "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"ALE/Pong-v5\" # doesn't exist for some reason: \"PongNoFrameskip-v4\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 10000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 1e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 1\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    buffer_size: int = 100 # 1000000\n",
    "    \"\"\"the replay memory buffer size\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    tau: float = 1.0\n",
    "    \"\"\"the target network update rate\"\"\"\n",
    "    target_network_frequency: int = 1000\n",
    "    \"\"\"the timesteps it takes to update the target network\"\"\"\n",
    "    batch_size: int = 32\n",
    "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
    "    start_e: float = 1\n",
    "    \"\"\"the starting epsilon for exploration\"\"\"\n",
    "    end_e: float = 0.01\n",
    "    \"\"\"the ending epsilon for exploration\"\"\"\n",
    "    exploration_fraction: float = 0.10\n",
    "    \"\"\"the fraction of `total-timesteps` it takes from start-e to go end-e\"\"\"\n",
    "    learning_starts: int = 80000\n",
    "    \"\"\"timestep to start learning\"\"\"\n",
    "    train_frequency: int = 4\n",
    "    \"\"\"the frequency of training\"\"\"\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0: \n",
    "            env = gym.make(env_id, obs_type=\"grayscale\") # env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id, obs_type=\"grayscale\") # env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = EpisodicLifeEnv(env)\n",
    "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "            env = FireResetEnv(env)\n",
    "        env = ClipRewardEnv(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        # env = gym.wrappers.GrayScaleObservation(env)\n",
    "        env = gym.wrappers.FrameStackObservation(env, 4) # env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.single_action_space.n),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x / 255.0)\n",
    "\n",
    "\n",
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return max(slope * t + start_e, end_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d81c31-2a56-49ab-83ca-8884ade1a3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 1022\n",
      "SPS: 1012\n",
      "SPS: 1003\n",
      "SPS: 994\n",
      "SPS: 985\n",
      "SPS: 976\n",
      "SPS: 967\n",
      "SPS: 959\n",
      "SPS: 950\n",
      "SPS: 942\n",
      "SPS: 933\n",
      "SPS: 926\n",
      "SPS: 918\n",
      "SPS: 910\n",
      "SPS: 903\n",
      "SPS: 895\n",
      "SPS: 885\n",
      "SPS: 876\n",
      "SPS: 868\n",
      "SPS: 861\n",
      "SPS: 853\n",
      "SPS: 843\n",
      "SPS: 835\n",
      "SPS: 826\n",
      "SPS: 817\n",
      "SPS: 806\n",
      "SPS: 799\n",
      "SPS: 792\n",
      "SPS: 787\n",
      "SPS: 781\n",
      "SPS: 776\n",
      "SPS: 771\n",
      "SPS: 765\n",
      "SPS: 760\n",
      "SPS: 755\n",
      "SPS: 750\n",
      "SPS: 745\n",
      "SPS: 741\n",
      "SPS: 736\n",
      "SPS: 731\n",
      "SPS: 726\n",
      "SPS: 722\n",
      "SPS: 717\n",
      "SPS: 713\n",
      "SPS: 709\n",
      "SPS: 705\n",
      "SPS: 700\n",
      "SPS: 696\n",
      "SPS: 692\n",
      "SPS: 688\n",
      "SPS: 684\n",
      "SPS: 680\n",
      "SPS: 677\n",
      "SPS: 673\n",
      "SPS: 669\n",
      "SPS: 665\n",
      "SPS: 661\n",
      "SPS: 658\n",
      "SPS: 654\n",
      "SPS: 651\n",
      "SPS: 648\n",
      "SPS: 644\n",
      "SPS: 641\n",
      "SPS: 637\n",
      "SPS: 634\n",
      "SPS: 631\n",
      "SPS: 628\n",
      "SPS: 624\n",
      "SPS: 621\n",
      "SPS: 618\n",
      "SPS: 615\n",
      "SPS: 612\n",
      "SPS: 609\n",
      "SPS: 605\n",
      "SPS: 602\n",
      "SPS: 600\n",
      "SPS: 597\n",
      "SPS: 594\n",
      "SPS: 591\n",
      "SPS: 588\n",
      "SPS: 586\n",
      "SPS: 583\n",
      "SPS: 581\n",
      "SPS: 578\n",
      "SPS: 575\n",
      "SPS: 573\n",
      "SPS: 570\n",
      "SPS: 567\n",
      "SPS: 562\n",
      "SPS: 558\n",
      "SPS: 554\n",
      "SPS: 550\n",
      "SPS: 547\n",
      "SPS: 545\n",
      "SPS: 542\n",
      "SPS: 539\n",
      "SPS: 537\n",
      "SPS: 534\n",
      "SPS: 531\n",
      "SPS: 529\n",
      "SPS: 526\n",
      "SPS: 524\n",
      "SPS: 522\n",
      "SPS: 520\n",
      "SPS: 518\n",
      "SPS: 516\n",
      "SPS: 514\n",
      "SPS: 512\n",
      "SPS: 510\n",
      "SPS: 508\n",
      "SPS: 506\n",
      "SPS: 504\n",
      "SPS: 503\n",
      "SPS: 501\n",
      "SPS: 499\n",
      "SPS: 497\n",
      "SPS: 496\n",
      "SPS: 494\n",
      "SPS: 492\n",
      "SPS: 491\n",
      "SPS: 489\n",
      "SPS: 487\n",
      "SPS: 486\n",
      "SPS: 484\n",
      "SPS: 482\n",
      "SPS: 481\n",
      "SPS: 479\n",
      "SPS: 478\n",
      "SPS: 476\n",
      "SPS: 475\n",
      "SPS: 473\n",
      "SPS: 472\n",
      "SPS: 470\n",
      "SPS: 468\n",
      "SPS: 467\n",
      "SPS: 466\n",
      "SPS: 464\n",
      "SPS: 463\n",
      "SPS: 461\n",
      "SPS: 460\n",
      "SPS: 458\n",
      "SPS: 457\n",
      "SPS: 456\n",
      "SPS: 454\n",
      "SPS: 453\n",
      "SPS: 451\n",
      "SPS: 450\n",
      "SPS: 449\n",
      "SPS: 447\n",
      "SPS: 445\n",
      "SPS: 443\n",
      "SPS: 442\n",
      "SPS: 440\n",
      "SPS: 439\n",
      "SPS: 438\n",
      "SPS: 436\n",
      "SPS: 435\n",
      "SPS: 434\n",
      "SPS: 432\n",
      "SPS: 431\n",
      "SPS: 430\n",
      "SPS: 429\n",
      "SPS: 427\n",
      "SPS: 426\n",
      "SPS: 425\n",
      "SPS: 424\n",
      "SPS: 423\n",
      "SPS: 422\n",
      "SPS: 420\n",
      "SPS: 419\n",
      "SPS: 418\n",
      "SPS: 417\n",
      "SPS: 416\n",
      "SPS: 415\n",
      "SPS: 414\n",
      "SPS: 413\n",
      "SPS: 412\n",
      "SPS: 411\n",
      "SPS: 410\n",
      "SPS: 409\n",
      "SPS: 408\n",
      "SPS: 406\n",
      "SPS: 406\n",
      "SPS: 404\n",
      "SPS: 403\n",
      "SPS: 402\n",
      "SPS: 402\n",
      "SPS: 401\n",
      "SPS: 400\n",
      "SPS: 399\n",
      "SPS: 398\n",
      "SPS: 397\n",
      "SPS: 396\n",
      "SPS: 395\n",
      "SPS: 394\n",
      "SPS: 393\n",
      "SPS: 392\n",
      "SPS: 391\n",
      "SPS: 390\n",
      "SPS: 389\n",
      "SPS: 388\n",
      "SPS: 387\n",
      "SPS: 386\n",
      "SPS: 386\n",
      "SPS: 385\n",
      "SPS: 384\n",
      "SPS: 383\n",
      "SPS: 382\n",
      "SPS: 381\n",
      "SPS: 380\n",
      "SPS: 380\n",
      "SPS: 379\n",
      "SPS: 378\n",
      "SPS: 377\n",
      "SPS: 376\n",
      "SPS: 376\n",
      "SPS: 375\n",
      "SPS: 374\n",
      "SPS: 373\n",
      "SPS: 372\n",
      "SPS: 371\n",
      "SPS: 371\n",
      "SPS: 370\n",
      "SPS: 369\n",
      "SPS: 368\n",
      "SPS: 368\n",
      "SPS: 367\n",
      "SPS: 366\n",
      "SPS: 365\n",
      "SPS: 365\n",
      "SPS: 364\n",
      "SPS: 363\n",
      "SPS: 362\n",
      "SPS: 362\n",
      "SPS: 361\n",
      "SPS: 360\n",
      "SPS: 359\n",
      "SPS: 359\n",
      "SPS: 358\n",
      "SPS: 357\n",
      "SPS: 357\n",
      "SPS: 356\n",
      "SPS: 355\n",
      "SPS: 354\n",
      "SPS: 354\n",
      "SPS: 353\n",
      "SPS: 352\n",
      "SPS: 351\n",
      "SPS: 351\n",
      "SPS: 350\n",
      "SPS: 349\n",
      "SPS: 349\n",
      "SPS: 348\n",
      "SPS: 348\n",
      "SPS: 347\n",
      "SPS: 346\n",
      "SPS: 345\n",
      "SPS: 345\n",
      "SPS: 344\n",
      "SPS: 344\n",
      "SPS: 343\n",
      "SPS: 342\n",
      "SPS: 342\n",
      "SPS: 341\n",
      "SPS: 340\n",
      "SPS: 340\n",
      "SPS: 339\n",
      "SPS: 339\n",
      "SPS: 338\n",
      "SPS: 337\n",
      "SPS: 337\n",
      "SPS: 336\n",
      "SPS: 336\n",
      "SPS: 335\n",
      "SPS: 334\n",
      "SPS: 334\n",
      "SPS: 333\n",
      "SPS: 333\n",
      "SPS: 332\n",
      "SPS: 332\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "\n",
    "args = Args()  # manually construct args inside notebooks\n",
    "\n",
    "assert args.num_envs == 1, \"vectorized envs are not supported at the moment\"\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "\n",
    "\n",
    "if \"ipykernel\" in sys.argv[0]:  # Running inside notebook or Jupyter\n",
    "    args = Args()  # just use defaults\n",
    "else:\n",
    "    args = tyro.cli(Args)    \n",
    "assert args.num_envs == 1, \"vectorized envs are not supported at the moment\"\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    ")\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "q_network = QNetwork(envs).to(device)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\n",
    "target_network = QNetwork(envs).to(device)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    device,\n",
    "    optimize_memory_usage=True,\n",
    "    handle_timeout_termination=False,\n",
    ")\n",
    "start_time = time.time()\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "obs, _ = envs.reset(seed=args.seed)\n",
    "for global_step in range(args.total_timesteps):\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, global_step)\n",
    "    if random.random() < epsilon:\n",
    "        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "    else:\n",
    "        q_values = q_network(torch.Tensor(obs).to(device))\n",
    "        actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "\n",
    "    # TRY NOT TO MODIFY: execute the game and log data.\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    if \"final_info\" in infos:\n",
    "        for info in infos[\"final_info\"]:\n",
    "            if info and \"episode\" in info:\n",
    "                print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "    real_next_obs = next_obs.copy()\n",
    "    for idx, trunc in enumerate(truncations):\n",
    "        if trunc:\n",
    "            real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "    rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "    obs = next_obs\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > args.learning_starts:\n",
    "        if global_step % args.train_frequency == 0:\n",
    "            data = rb.sample(args.batch_size)\n",
    "            with torch.no_grad():\n",
    "                target_max, _ = target_network(data.next_observations).max(dim=1)\n",
    "                td_target = data.rewards.flatten() + args.gamma * target_max * (1 - data.dones.flatten())\n",
    "            old_val = q_network(data.observations).gather(1, data.actions).squeeze()\n",
    "            loss = F.mse_loss(td_target, old_val)\n",
    "\n",
    "            if global_step % 100 == 0:\n",
    "                writer.add_scalar(\"losses/td_loss\", loss, global_step)\n",
    "                writer.add_scalar(\"losses/q_values\", old_val.mean().item(), global_step)\n",
    "                print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "                writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "            # optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # update target network\n",
    "        if global_step % args.target_network_frequency == 0:\n",
    "            for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
    "                target_network_param.data.copy_(\n",
    "                    args.tau * q_network_param.data + (1.0 - args.tau) * target_network_param.data\n",
    "                )\n",
    "\n",
    "if args.save_model:\n",
    "    model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
    "    torch.save(q_network.state_dict(), model_path)\n",
    "    print(f\"model saved to {model_path}\")\n",
    "    from cleanrl_utils.evals.dqn_eval import evaluate\n",
    "\n",
    "    episodic_returns = evaluate(\n",
    "        model_path,\n",
    "        make_env,\n",
    "        args.env_id,\n",
    "        eval_episodes=10,\n",
    "        run_name=f\"{run_name}-eval\",\n",
    "        Model=QNetwork,\n",
    "        device=device,\n",
    "        epsilon=args.end_e,\n",
    "    )\n",
    "    for idx, episodic_return in enumerate(episodic_returns):\n",
    "        writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
    "\n",
    "    if args.upload_model:\n",
    "        from cleanrl_utils.huggingface import push_to_hub\n",
    "\n",
    "        repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
    "        repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
    "        push_to_hub(args, episodic_returns, repo_id, \"DQN\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
    "\n",
    "envs.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e488f26-0ec4-44c9-9740-b416992e2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
