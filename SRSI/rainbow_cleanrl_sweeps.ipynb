{"cells":[{"cell_type":"code","execution_count":null,"id":"96effb5f-bfb1-406c-9e41-70ff63801c40","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72172,"status":"ok","timestamp":1754373749451,"user":{"displayName":"SRSI","userId":"05291714822278967909"},"user_tz":-180},"id":"96effb5f-bfb1-406c-9e41-70ff63801c40","outputId":"b3d3f078-72d2-4912-b821-0afd04386fa8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.2.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Collecting tyro\n","  Downloading tyro-0.9.27-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.11.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n","Collecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro) (0.17.0)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (13.9.4)\n","Collecting shtab>=1.5.6 (from tyro)\n","  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro) (4.4.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.74.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro) (2.19.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro) (0.1.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.9.27-py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n","Installing collected packages: shtab, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorama, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed colorama-0.4.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 shtab-1.7.2 tyro-0.9.27\n"]}],"source":["!pip3 install numpy gymnasium torch torchvision torchaudio tyro tensorboard ale-py psutil opencv-python colorama"]},{"cell_type":"code","execution_count":null,"id":"65f24384-139b-4045-81ff-a292819adfa5","metadata":{"id":"65f24384-139b-4045-81ff-a292819adfa5"},"outputs":[],"source":["# imports\n","from __future__ import annotations\n","\n","import os\n","import random\n","import sys\n","import time\n","from dataclasses import dataclass\n","import warnings\n","from abc import ABC, abstractmethod\n","from collections.abc import Generator\n","from typing import Any, NamedTuple, SupportsFloat, List, Callable\n","import wandb\n","\n","import numpy as np\n","from gymnasium import spaces\n","import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import tyro\n","from torch.utils.tensorboard import SummaryWriter\n","import ale_py\n","\n","import argparse\n","from pathlib import Path\n","from pprint import pformat\n","from tenacity import retry, stop_after_attempt, wait_fixed\n","import random\n","import math\n","import collections\n","from collections import deque\n","\n","\n","\n","try:\n","    # Check memory used by replay buffer when possible\n","    import psutil\n","except ImportError:\n","    psutil = None\n","\n","try:\n","    import cv2\n","\n","    cv2.ocl.setUseOpenCL(False)\n","except ImportError:\n","    cv2 = None  # type: ignore[assignment]\n"]},{"cell_type":"code","execution_count":null,"id":"T43h40ff_MSx","metadata":{"id":"T43h40ff_MSx"},"outputs":[],"source":["def evaluate(\n","    model_path: str,\n","    make_env: Callable,\n","    env_id: str,\n","    eval_episodes: int,\n","    run_name: str,\n","    Model: torch.nn.Module,\n","    device: torch.device = torch.device(\"cpu\"),\n","    epsilon: float = 0.05,\n","    capture_video: bool = False, # capture_video: bool = True,\n","):\n","    envs = gym.vector.SyncVectorEnv([make_env(env_id, 0, 0, capture_video, run_name)])\n","    model = Model(envs).to(device)\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.eval()\n","\n","    obs, _ = envs.reset()\n","    episodic_returns = []\n","    while len(episodic_returns) < eval_episodes:\n","        if random.random() < epsilon:\n","            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n","        else:\n","            q_values = model(torch.Tensor(obs).to(device))\n","            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n","        next_obs, _, _, _, infos = envs.step(actions)\n","        if \"final_info\" in infos:\n","            for info in infos[\"final_info\"]:\n","                if \"episode\" not in info:\n","                    continue\n","                print(f\"eval_episode={len(episodic_returns)}, episodic_return={info['episode']['r']}\")\n","                episodic_returns += [info[\"episode\"][\"r\"]]\n","        obs = next_obs\n","\n","    return episodic_returns"]},{"cell_type":"code","execution_count":null,"id":"jtfkZwAs-0qw","metadata":{"id":"jtfkZwAs-0qw"},"outputs":[],"source":["# hugging face\n","\n","HUGGINGFACE_VIDEO_PREVIEW_FILE_NAME = \"replay.mp4\"\n","HUGGINGFACE_README_FILE_NAME = \"README.md\"\n","\n","\n","@retry(stop=stop_after_attempt(10), wait=wait_fixed(3))\n","def push_to_hub(\n","    args: argparse.Namespace,\n","    episodic_returns: List,\n","    repo_id: str,\n","    algo_name: str,\n","    folder_path: str,\n","    video_folder_path: str = \"\",\n","    revision: str = \"main\",\n","    create_pr: bool = False,\n","    private: bool = False,\n","):\n","    # Step 1: lazy import and create / read a huggingface repo\n","    from huggingface_hub import CommitOperationAdd, CommitOperationDelete, HfApi\n","    from huggingface_hub.repocard import metadata_eval_result, metadata_save\n","\n","    api = HfApi()\n","    repo_url = api.create_repo(\n","        repo_id=repo_id,\n","        exist_ok=True,\n","        private=private,\n","    )\n","    # parse the default entity\n","    entity, repo = repo_url.split(\"/\")[-2:]\n","    repo_id = f\"{entity}/{repo}\"\n","\n","    # Step 2: clean up data\n","    # delete previous tfevents and mp4 files\n","    operations = [\n","        CommitOperationDelete(path_in_repo=file)\n","        for file in api.list_repo_files(repo_id=repo_id)\n","        if \".tfevents\" in file or file.endswith(\".mp4\")\n","    ]\n","\n","    # Step 3: Generate the model card\n","    algorithm_variant_filename = sys.argv[0].split(\"/\")[-1]\n","    model_card = f\"\"\"\n","# (CleanRL) **{algo_name}** Agent Playing **{args.env_id}**\n","\n","This is a trained model of a {algo_name} agent playing {args.env_id}.\n","The model was trained by using [CleanRL](https://github.com/vwxyzjn/cleanrl) and the most up-to-date training code can be\n","found [here](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/{args.exp_name}.py).\n","\n","## Get Started\n","\n","To use this model, please install the `cleanrl` package with the following command:\n","\n","```\n","pip install \"cleanrl[{args.exp_name}]\"\n","python -m cleanrl_utils.enjoy --exp-name {args.exp_name} --env-id {args.env_id}\n","```\n","\n","Please refer to the [documentation](https://docs.cleanrl.dev/get-started/zoo/) for more detail.\n","\n","\n","## Command to reproduce the training\n","\n","```bash\n","curl -OL https://huggingface.co/{repo_id}/raw/main/{algorithm_variant_filename}\n","curl -OL https://huggingface.co/{repo_id}/raw/main/pyproject.toml\n","curl -OL https://huggingface.co/{repo_id}/raw/main/poetry.lock\n","poetry install --all-extras\n","python {algorithm_variant_filename} {\" \".join(sys.argv[1:])}\n","```\n","\n","# Hyperparameters\n","```python\n","{pformat(vars(args))}\n","```\n","    \"\"\"\n","    readme_path = Path(folder_path) / HUGGINGFACE_README_FILE_NAME\n","    readme = model_card\n","\n","    # metadata\n","    metadata = {}\n","    metadata[\"tags\"] = [\n","        args.env_id,\n","        \"deep-reinforcement-learning\",\n","        \"reinforcement-learning\",\n","        \"custom-implementation\",\n","    ]\n","    metadata[\"library_name\"] = \"cleanrl\"\n","    eval = metadata_eval_result(\n","        model_pretty_name=algo_name,\n","        task_pretty_name=\"reinforcement-learning\",\n","        task_id=\"reinforcement-learning\",\n","        metrics_pretty_name=\"mean_reward\",\n","        metrics_id=\"mean_reward\",\n","        metrics_value=f\"{np.average(episodic_returns):.2f} +/- {np.std(episodic_returns):.2f}\",\n","        dataset_pretty_name=args.env_id,\n","        dataset_id=args.env_id,\n","    )\n","    metadata = {**metadata, **eval}\n","\n","    with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(readme)\n","    metadata_save(readme_path, metadata)\n","\n","    # fetch mp4 files\n","    if video_folder_path:\n","        # Push all video files\n","        video_files = list(Path(video_folder_path).glob(\"*.mp4\"))\n","        operations += [CommitOperationAdd(path_or_fileobj=str(file), path_in_repo=str(file)) for file in video_files]\n","        # Push latest one in root directory\n","        latest_file = max(video_files, key=lambda file: int(\"\".join(filter(str.isdigit, file.stem))))\n","        operations.append(\n","            CommitOperationAdd(path_or_fileobj=str(latest_file), path_in_repo=HUGGINGFACE_VIDEO_PREVIEW_FILE_NAME)\n","        )\n","\n","    # fetch folder files\n","    operations += [\n","        CommitOperationAdd(path_or_fileobj=str(item), path_in_repo=str(item.relative_to(folder_path)))\n","        for item in Path(folder_path).glob(\"*\")\n","    ]\n","\n","    # fetch source code\n","    operations.append(CommitOperationAdd(path_or_fileobj=sys.argv[0], path_in_repo=sys.argv[0].split(\"/\")[-1]))\n","\n","    # upload poetry files at the root of the repository\n","    git_root = Path(__file__).parent.parent\n","    operations.append(CommitOperationAdd(path_or_fileobj=str(git_root / \"pyproject.toml\"), path_in_repo=\"pyproject.toml\"))\n","    operations.append(CommitOperationAdd(path_or_fileobj=str(git_root / \"poetry.lock\"), path_in_repo=\"poetry.lock\"))\n","\n","    api.create_commit(\n","        repo_id=repo_id,\n","        operations=operations,\n","        commit_message=\"pushing model\",\n","        revision=revision,\n","        create_pr=create_pr,\n","    )\n","    print(f\"Model pushed to {repo_url}\")\n","    return repo_url"]},{"cell_type":"code","execution_count":null,"id":"da6946d4-9ffd-410d-b2a9-989b85f390fb","metadata":{"id":"da6946d4-9ffd-410d-b2a9-989b85f390fb"},"outputs":[],"source":["# from cleanrl_utils.buffers import ReplayBuffer\n","\n","class ReplayBufferSamples(NamedTuple):\n","    observations: torch.Tensor\n","    actions: torch.Tensor\n","    next_observations: torch.Tensor\n","    dones: torch.Tensor\n","    rewards: torch.Tensor\n","\n","\n","def get_device(device: torch.device | str = \"auto\") -> torch.device:\n","    \"\"\"\n","    Retrieve PyTorch device.\n","    It checks that the requested device is available first.\n","    For now, it supports only cpu and cuda.\n","    By default, it tries to use the gpu.\n","\n","    :param device: One for 'auto', 'cuda', 'cpu'\n","    :return: Supported Pytorch device\n","    \"\"\"\n","    # Cuda by default\n","    if device == \"auto\":\n","        device = \"cuda\"\n","    # Force conversion to torch.device\n","    device = torch.device(device)\n","\n","    # Cuda not available\n","    if device.type == torch.device(\"cuda\").type and not torch.cuda.is_available():\n","        return torch.device(\"cpu\")\n","\n","    return device\n","\n","\n","def get_obs_shape(\n","    observation_space: spaces.Space,\n",") -> tuple[int, ...] | dict[str, tuple[int, ...]]:\n","    \"\"\"\n","    Get the shape of the observation (useful for the buffers).\n","\n","    :param observation_space:\n","    :return:\n","    \"\"\"\n","    if isinstance(observation_space, spaces.Box):\n","        return observation_space.shape\n","    elif isinstance(observation_space, spaces.Discrete):\n","        # Observation is an int\n","        return (1,)\n","    elif isinstance(observation_space, spaces.MultiDiscrete):\n","        # Number of discrete features\n","        return (int(len(observation_space.nvec)),)\n","    elif isinstance(observation_space, spaces.MultiBinary):\n","        # Number of binary features\n","        return observation_space.shape\n","    elif isinstance(observation_space, spaces.Dict):\n","        return {key: get_obs_shape(subspace) for (key, subspace) in observation_space.spaces.items()}  # type: ignore[misc]\n","\n","    else:\n","        raise NotImplementedError(f\"{observation_space} observation space is not supported\")\n","\n","def get_action_dim(action_space: spaces.Space) -> int:\n","    \"\"\"\n","    Get the dimension of the action space.\n","\n","    :param action_space:\n","    :return:\n","    \"\"\"\n","    if isinstance(action_space, spaces.Box):\n","        return int(np.prod(action_space.shape))\n","    elif isinstance(action_space, spaces.Discrete):\n","        # Action is an int\n","        return 1\n","    elif isinstance(action_space, spaces.MultiDiscrete):\n","        # Number of discrete actions\n","        return int(len(action_space.nvec))\n","    elif isinstance(action_space, spaces.MultiBinary):\n","        # Number of binary actions\n","        assert isinstance(\n","            action_space.n, int\n","        ), f\"Multi-dimensional MultiBinary({action_space.n}) action space is not supported. You can flatten it instead.\"\n","        return int(action_space.n)\n","    else:\n","        raise NotImplementedError(f\"{action_space} action space is not supported\")\n","\n","\n","\n","class BaseBuffer(ABC):\n","    \"\"\"\n","    Base class that represent a buffer (rollout or replay)\n","\n","    :param buffer_size: Max number of element in the buffer\n","    :param observation_space: Observation space\n","    :param action_space: Action space\n","    :param device: PyTorch device\n","        to which the values will be converted\n","    :param n_envs: Number of parallel environments\n","    \"\"\"\n","\n","    observation_space: spaces.Space\n","    obs_shape: tuple[int, ...]\n","\n","    def __init__(\n","        self,\n","        buffer_size: int,\n","        observation_space: spaces.Space,\n","        action_space: spaces.Space,\n","        device: torch.device | str = \"auto\",\n","        n_envs: int = 1,\n","    ):\n","        super().__init__()\n","        self.buffer_size = buffer_size\n","        self.observation_space = observation_space\n","        self.action_space = action_space\n","        self.obs_shape = get_obs_shape(observation_space)  # type: ignore[assignment]\n","\n","        self.action_dim = get_action_dim(action_space)\n","        self.pos = 0\n","        self.full = False\n","        self.device = get_device(device)\n","        self.n_envs = n_envs\n","\n","    @staticmethod\n","    def swap_and_flatten(arr: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Swap and then flatten axes 0 (buffer_size) and 1 (n_envs)\n","        to convert shape from [n_steps, n_envs, ...] (when ... is the shape of the features)\n","        to [n_steps * n_envs, ...] (which maintain the order)\n","\n","        :param arr:\n","        :return:\n","        \"\"\"\n","        shape = arr.shape\n","        if len(shape) < 3:\n","            shape = (*shape, 1)\n","        return arr.swapaxes(0, 1).reshape(shape[0] * shape[1], *shape[2:])\n","\n","    def size(self) -> int:\n","        \"\"\"\n","        :return: The current size of the buffer\n","        \"\"\"\n","        if self.full:\n","            return self.buffer_size\n","        return self.pos\n","\n","    def add(self, *args, **kwargs) -> None:\n","        \"\"\"\n","        Add elements to the buffer.\n","        \"\"\"\n","        raise NotImplementedError()\n","\n","    def extend(self, *args, **kwargs) -> None:\n","        \"\"\"\n","        Add a new batch of transitions to the buffer\n","        \"\"\"\n","        # Do a for loop along the batch axis\n","        for data in zip(*args):\n","            self.add(*data)\n","\n","    def reset(self) -> None:\n","        \"\"\"\n","        Reset the buffer.\n","        \"\"\"\n","        self.pos = 0\n","        self.full = False\n","\n","    def sample(self, batch_size: int):\n","        \"\"\"\n","        :param batch_size: Number of element to sample\n","        :return:\n","        \"\"\"\n","        upper_bound = self.buffer_size if self.full else self.pos\n","        batch_inds = np.random.randint(0, upper_bound, size=batch_size)\n","        return self._get_samples(batch_inds)\n","\n","    @abstractmethod\n","    def _get_samples(self, batch_inds: np.ndarray) -> ReplayBufferSamples | RolloutBufferSamples:\n","        \"\"\"\n","        :param batch_inds:\n","        :return:\n","        \"\"\"\n","        raise NotImplementedError()\n","\n","    def to_torch(self, array: np.ndarray, copy: bool = True) -> torch.Tensor:\n","        \"\"\"\n","        Convert a numpy array to a PyTorch tensor.\n","        Note: it copies the data by default\n","\n","        :param array:\n","        :param copy: Whether to copy or not the data (may be useful to avoid changing things\n","            by reference). This argument is inoperative if the device is not the CPU.\n","        :return:\n","        \"\"\"\n","        if copy:\n","            return torch.tensor(array, device=self.device)\n","        return torch.as_tensor(array, device=self.device)\n","\n","\n","class ReplayBuffer(BaseBuffer):\n","    \"\"\"\n","    Replay buffer used in off-policy algorithms like SAC/TD3.\n","\n","    :param buffer_size: Max number of element in the buffer\n","    :param observation_space: Observation space\n","    :param action_space: Action space\n","    :param device: PyTorch device\n","    :param n_envs: Number of parallel environments\n","    :param optimize_memory_usage: Enable a memory efficient variant\n","        of the replay buffer which reduces by almost a factor two the memory used,\n","        at a cost of more complexity.\n","        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n","        and https://github.com/DLR-RM/stable-baselines3/pull/28#issuecomment-637559274\n","        Cannot be used in combination with handle_timeout_termination.\n","    :param handle_timeout_termination: Handle timeout termination (due to timelimit)\n","        separately and treat the task as infinite horizon task.\n","        https://github.com/DLR-RM/stable-baselines3/issues/284\n","    \"\"\"\n","\n","    observations: np.ndarray\n","    next_observations: np.ndarray\n","    actions: np.ndarray\n","    rewards: np.ndarray\n","    dones: np.ndarray\n","    timeouts: np.ndarray\n","\n","    def __init__(\n","        self,\n","        buffer_size: int,\n","        observation_space: spaces.Space,\n","        action_space: spaces.Space,\n","        device: torch.device | str = \"auto\",\n","        n_envs: int = 1,\n","        optimize_memory_usage: bool = False,\n","        handle_timeout_termination: bool = True,\n","    ):\n","        super().__init__(buffer_size, observation_space, action_space, device, n_envs=n_envs)\n","\n","        # Adjust buffer size\n","        self.buffer_size = max(buffer_size // n_envs, 1)\n","\n","        # Check that the replay buffer can fit into the memory\n","        if psutil is not None:\n","            mem_available = psutil.virtual_memory().available\n","\n","        # there is a bug if both optimize_memory_usage and handle_timeout_termination are true\n","        # see https://github.com/DLR-RM/stable-baselines3/issues/934\n","        if optimize_memory_usage and handle_timeout_termination:\n","            raise ValueError(\n","                \"ReplayBuffer does not support optimize_memory_usage = True \"\n","                \"and handle_timeout_termination = True simultaneously.\"\n","            )\n","        self.optimize_memory_usage = optimize_memory_usage\n","\n","        self.observations = np.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=observation_space.dtype)\n","\n","        if not optimize_memory_usage:\n","            # When optimizing memory, `observations` contains also the next observation\n","            self.next_observations = np.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=observation_space.dtype)\n","\n","        self.actions = np.zeros(\n","            (self.buffer_size, self.n_envs, self.action_dim), dtype=self._maybe_cast_dtype(action_space.dtype)\n","        )\n","\n","        self.rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n","        self.dones = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n","        # Handle timeouts termination properly if needed\n","        # see https://github.com/DLR-RM/stable-baselines3/issues/284\n","        self.handle_timeout_termination = handle_timeout_termination\n","        self.timeouts = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n","\n","        if psutil is not None:\n","            total_memory_usage: float = (\n","                self.observations.nbytes + self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n","            )\n","\n","            if not optimize_memory_usage:\n","                total_memory_usage += self.next_observations.nbytes\n","\n","            if total_memory_usage > mem_available:\n","                # Convert to GB\n","                total_memory_usage /= 1e9\n","                mem_available /= 1e9\n","                warnings.warn(\n","                    \"This system does not have apparently enough memory to store the complete \"\n","                    f\"replay buffer {total_memory_usage:.2f}GB > {mem_available:.2f}GB\"\n","                )\n","\n","    def add(\n","        self,\n","        obs: np.ndarray,\n","        next_obs: np.ndarray,\n","        action: np.ndarray,\n","        reward: np.ndarray,\n","        done: np.ndarray,\n","        infos: list[dict[str, Any]],\n","    ) -> None:\n","        # Reshape needed when using multiple envs with discrete observations\n","        # as numpy cannot broadcast (n_discrete,) to (n_discrete, 1)\n","        if isinstance(self.observation_space, spaces.Discrete):\n","            obs = obs.reshape((self.n_envs, *self.obs_shape))\n","            next_obs = next_obs.reshape((self.n_envs, *self.obs_shape))\n","\n","        # Reshape to handle multi-dim and discrete action spaces, see GH #970 #1392\n","        action = action.reshape((self.n_envs, self.action_dim))\n","\n","        # Copy to avoid modification by reference\n","        self.observations[self.pos] = np.array(obs)\n","\n","        if self.optimize_memory_usage:\n","            self.observations[(self.pos + 1) % self.buffer_size] = np.array(next_obs)\n","        else:\n","            self.next_observations[self.pos] = np.array(next_obs)\n","\n","        self.actions[self.pos] = np.array(action)\n","        self.rewards[self.pos] = np.array(reward)\n","        self.dones[self.pos] = np.array(done)\n","\n","        if self.handle_timeout_termination:\n","            self.timeouts[self.pos] = np.array([info.get(\"TimeLimit.truncated\", False) for info in infos])\n","\n","        self.pos += 1\n","        if self.pos == self.buffer_size:\n","            self.full = True\n","            self.pos = 0\n","\n","    def sample(self, batch_size: int) -> ReplayBufferSamples:\n","        \"\"\"\n","        Sample elements from the replay buffer.\n","        Custom sampling when using memory efficient variant,\n","        as we should not sample the element with index `self.pos`\n","        See https://github.com/DLR-RM/stable-baselines3/pull/28#issuecomment-637559274\n","\n","        :param batch_size: Number of element to sample\n","        :return:\n","        \"\"\"\n","        if not self.optimize_memory_usage:\n","            return super().sample(batch_size=batch_size)\n","        # Do not sample the element with index `self.pos` as the transitions is invalid\n","        # (we use only one array to store `obs` and `next_obs`)\n","        if self.full:\n","            batch_inds = (np.random.randint(1, self.buffer_size, size=batch_size) + self.pos) % self.buffer_size\n","        else:\n","            batch_inds = np.random.randint(0, self.pos, size=batch_size)\n","        return self._get_samples(batch_inds)\n","\n","    def _get_samples(self, batch_inds: np.ndarray) -> ReplayBufferSamples:\n","        # Sample randomly the env idx\n","        env_indices = np.random.randint(0, high=self.n_envs, size=(len(batch_inds),))\n","\n","        if self.optimize_memory_usage:\n","            next_obs = self.observations[(batch_inds + 1) % self.buffer_size, env_indices, :]\n","        else:\n","            next_obs = self.next_observations[batch_inds, env_indices, :]\n","\n","        data = (\n","            self.observations[batch_inds, env_indices, :],\n","            self.actions[batch_inds, env_indices, :],\n","            next_obs,\n","            # Only use dones that are not due to timeouts\n","            # deactivated by default (timeouts is initialized as an array of False)\n","            (self.dones[batch_inds, env_indices] * (1 - self.timeouts[batch_inds, env_indices])).reshape(-1, 1),\n","            self.rewards[batch_inds, env_indices].reshape(-1, 1),\n","        )\n","        return ReplayBufferSamples(*tuple(map(self.to_torch, data)))\n","\n","    @staticmethod\n","    def _maybe_cast_dtype(dtype: np.typing.DTypeLike) -> np.typing.DTypeLike:\n","        \"\"\"\n","        Cast `np.float64` action datatype to `np.float32`,\n","        keep the others dtype unchanged.\n","        See GH#1572 for more information.\n","\n","        :param dtype: The original action space dtype\n","        :return: ``np.float32`` if the dtype was float64,\n","            the original dtype otherwise.\n","        \"\"\"\n","        if dtype == np.float64:\n","            return np.float32\n","        return dtype\n"]},{"cell_type":"code","execution_count":null,"id":"4f7bf75d-8cb2-4b94-bcb3-bc2d8738cb3a","metadata":{"id":"4f7bf75d-8cb2-4b94-bcb3-bc2d8738cb3a"},"outputs":[],"source":["class ClipRewardEnv(gym.RewardWrapper):\n","    \"\"\"\n","    Clip the reward to {+1, 0, -1} by its sign.\n","\n","    :param env: Environment to wrap\n","    \"\"\"\n","\n","    def __init__(self, env: gym.Env) -> None:\n","        super().__init__(env)\n","\n","    def reward(self, reward: SupportsFloat) -> float:\n","        \"\"\"\n","        Bin reward to {+1, 0, -1} by its sign.\n","\n","        :param reward:\n","        :return:\n","        \"\"\"\n","        return np.sign(float(reward))\n","\n","\n","class EpisodicLifeEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n","    \"\"\"\n","    Make end-of-life == end-of-episode, but only reset on true game over.\n","    Done by DeepMind for the DQN and co. since it helps value estimation.\n","\n","    :param env: Environment to wrap\n","    \"\"\"\n","\n","    def __init__(self, env: gym.Env) -> None:\n","        super().__init__(env)\n","        self.lives = 0\n","        self.was_real_done = True\n","\n","    def step(self, action: int):\n","        obs, reward, terminated, truncated, info = self.env.step(action)\n","        self.was_real_done = terminated or truncated\n","        # check current lives, make loss of life terminal,\n","        # then update lives to handle bonus lives\n","        lives = self.env.unwrapped.ale.lives()  # type: ignore[attr-defined]\n","        if 0 < lives < self.lives:\n","            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n","            # so its important to keep lives > 0, so that we only reset once\n","            # the environment advertises done.\n","            terminated = True\n","        self.lives = lives\n","        return obs, reward, terminated, truncated, info\n","\n","    def reset(self, **kwargs):\n","        \"\"\"\n","        Calls the Gym environment reset, only when lives are exhausted.\n","        This way all states are still reachable even though lives are episodic,\n","        and the learner need not know about any of this behind-the-scenes.\n","\n","        :param kwargs: Extra keywords passed to env.reset() call\n","        :return: the first observation of the environment\n","        \"\"\"\n","        if self.was_real_done:\n","            obs, info = self.env.reset(**kwargs)\n","        else:\n","            # no-op step to advance from terminal/lost life state\n","            obs, _, terminated, truncated, info = self.env.step(0)\n","\n","            # The no-op step can lead to a game over, so we need to check it again\n","            # to see if we should reset the environment and avoid the\n","            # monitor.py `RuntimeError: Tried to step environment that needs reset`\n","            if terminated or truncated:\n","                obs, info = self.env.reset(**kwargs)\n","        self.lives = self.env.unwrapped.ale.lives()  # type: ignore[attr-defined]\n","        return obs, info\n","\n","class FireResetEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n","    \"\"\"\n","    Take action on reset for environments that are fixed until firing.\n","\n","    :param env: Environment to wrap\n","    \"\"\"\n","\n","    def __init__(self, env: gym.Env) -> None:\n","        super().__init__(env)\n","        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"  # type: ignore[attr-defined]\n","        assert len(env.unwrapped.get_action_meanings()) >= 3  # type: ignore[attr-defined]\n","\n","    def reset(self, **kwargs):\n","        self.env.reset(**kwargs)\n","        obs, _, terminated, truncated, _ = self.env.step(1)\n","        if terminated or truncated:\n","            self.env.reset(**kwargs)\n","        obs, _, terminated, truncated, _ = self.env.step(2)\n","        if terminated or truncated:\n","            self.env.reset(**kwargs)\n","        return obs, {}\n","\n","\n","class MaxAndSkipEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n","    \"\"\"\n","    Return only every ``skip``-th frame (frameskipping)\n","    and return the max between the two last frames.\n","\n","    :param env: Environment to wrap\n","    :param skip: Number of ``skip``-th frame\n","        The same action will be taken ``skip`` times.\n","    \"\"\"\n","\n","    def __init__(self, env: gym.Env, skip: int = 4) -> None:\n","        super().__init__(env)\n","        # most recent raw observations (for max pooling across time steps)\n","        assert env.observation_space.dtype is not None, \"No dtype specified for the observation space\"\n","        assert env.observation_space.shape is not None, \"No shape defined for the observation space\"\n","        self._obs_buffer = np.zeros((2, *env.observation_space.shape), dtype=env.observation_space.dtype)\n","        self._skip = skip\n","\n","    def step(self, action: int):\n","        \"\"\"\n","        Step the environment with the given action\n","        Repeat action, sum reward, and max over last observations.\n","\n","        :param action: the action\n","        :return: observation, reward, terminated, truncated, information\n","        \"\"\"\n","        total_reward = 0.0\n","        terminated = truncated = False\n","        for i in range(self._skip):\n","            obs, reward, terminated, truncated, info = self.env.step(action)\n","            done = terminated or truncated\n","            if i == self._skip - 2:\n","                self._obs_buffer[0] = obs\n","            if i == self._skip - 1:\n","                self._obs_buffer[1] = obs\n","            total_reward += float(reward)\n","            if done:\n","                break\n","        # Note that the observation on the done=True frame\n","        # doesn't matter\n","        max_frame = self._obs_buffer.max(axis=0)\n","\n","        return max_frame, total_reward, terminated, truncated, info\n","\n","class NoopResetEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n","    \"\"\"\n","    Sample initial states by taking random number of no-ops on reset.\n","    No-op is assumed to be action 0.\n","\n","    :param env: Environment to wrap\n","    :param noop_max: Maximum value of no-ops to run\n","    \"\"\"\n","\n","    def __init__(self, env: gym.Env, noop_max: int = 30) -> None:\n","        super().__init__(env)\n","        self.noop_max = noop_max\n","        self.override_num_noops = None\n","        self.noop_action = 0\n","        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"  # type: ignore[attr-defined]\n","\n","    def reset(self, **kwargs):\n","        self.env.reset(**kwargs)\n","        if self.override_num_noops is not None:\n","            noops = self.override_num_noops\n","        else:\n","            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n","        assert noops > 0\n","        obs = np.zeros(0)\n","        info: dict = {}\n","        for _ in range(noops):\n","            obs, _, terminated, truncated, info = self.env.step(self.noop_action)\n","            if terminated or truncated:\n","                obs, info = self.env.reset(**kwargs)\n","        return obs, info\n"]},{"cell_type":"code","execution_count":null,"id":"0ddffb28-7405-414a-b676-9db48c003104","metadata":{"id":"0ddffb28-7405-414a-b676-9db48c003104"},"outputs":[],"source":["try:\n","    script_name: str = os.path.basename(__file__)[: -len(\".py\")]\n","except NameError:\n","    script_name: str = \"notebook_run\"\n","\n","@dataclass\n","class Args:\n","    exp_name: str = script_name\n","    \"\"\"the name of this experiment\"\"\"\n","    seed: int = 1\n","    \"\"\"seed of the experiment\"\"\"\n","    torch_deterministic: bool = True\n","    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n","    cuda: bool = True\n","    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n","    track: bool = True\n","    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n","    wandb_project_name: str = \"cleanRL\"\n","    \"\"\"the wandb's project name\"\"\"\n","    wandb_entity: str = None\n","    \"\"\"the entity (team) of wandb's project\"\"\"\n","    capture_video: bool = False\n","    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n","    save_model: bool = True\n","    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n","    upload_model: bool = False\n","    \"\"\"whether to upload the saved model to huggingface\"\"\"\n","    hf_entity: str = \"\"\n","    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n","\n","    env_id: str = \"ALE/Pong-v5\"\n","    \"\"\"the id of the environment\"\"\"\n","    total_timesteps: int = 10000000 #10000000\n","    \"\"\"total timesteps of the experiments\"\"\"\n","    learning_rate: float = 1e-4 # 0.0000625\n","    \"\"\"the learning rate of the optimizer\"\"\"\n","    num_envs: int = 1\n","    \"\"\"the number of parallel game environments\"\"\"\n","    buffer_size: int = 500000 #1000000\n","    \"\"\"the replay memory buffer size\"\"\"\n","    gamma: float = 0.99\n","    \"\"\"the discount factor gamma\"\"\"\n","    tau: float = 1.0\n","    \"\"\"the target network update rate\"\"\"\n","    target_network_frequency: int = 8000\n","    \"\"\"the timesteps it takes to update the target network\"\"\"\n","    batch_size: int = 32\n","    \"\"\"the batch size of sample from the reply memory\"\"\"\n","    start_e: float = 0 # 1\n","    \"\"\"the starting epsilon for exploration\"\"\"\n","    end_e: float = 0 # 0.01\n","    \"\"\"the ending epsilon for exploration\"\"\"\n","    exploration_fraction: float = 0 # 0.1\n","    \"\"\"the fraction of `total-timesteps` it takes from start-e to go end-e\"\"\"\n","    learning_starts: int = 80000\n","    \"\"\"timestep to start learning\"\"\"\n","    train_frequency: int = 4\n","    \"\"\"the frequency of training\"\"\"\n","    n_step: int = 3\n","    \"\"\"the number of steps to look ahead for n-step Q learning\"\"\"\n","    prioritized_replay_alpha: float = 0.5\n","    \"\"\"alpha parameter for prioritized replay buffer\"\"\"\n","    prioritized_replay_beta: float = 0.4\n","    \"\"\"beta parameter for prioritized replay buffer\"\"\"\n","    prioritized_replay_eps: float = 1e-6\n","    \"\"\"epsilon parameter for prioritized replay buffer\"\"\"\n","    n_atoms: int = 51\n","    \"\"\"the number of atoms\"\"\"\n","    v_min: float = -10\n","    \"\"\"the return lower bound\"\"\"\n","    v_max: float = 10\n","    \"\"\"the return upper bound\"\"\"\n","\n","\n","def make_env(env_id, seed, idx, capture_video, run_name):\n","    def thunk():\n","        if capture_video and idx == 0:\n","            env = gym.make(env_id, obs_type=\"grayscale\") # env = gym.make(env_id, render_mode=\"rgb_array\")\n","            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n","        else:\n","            env = gym.make(env_id, obs_type=\"grayscale\") # env = gym.make(env_id)\n","        env = gym.wrappers.RecordEpisodeStatistics(env)\n","\n","        env = NoopResetEnv(env, noop_max=30)\n","        env = MaxAndSkipEnv(env, skip=4)\n","        env = EpisodicLifeEnv(env)\n","        if \"FIRE\" in env.unwrapped.get_action_meanings():\n","            env = FireResetEnv(env)\n","        env = ClipRewardEnv(env)\n","        env = gym.wrappers.ResizeObservation(env, (84, 84))\n","        # env = gym.wrappers.GrayScaleObservation(env)\n","        env = gym.wrappers.FrameStackObservation(env, 4) # env = gym.wrappers.FrameStack(env, 4)\n","\n","        env.action_space.seed(seed)\n","        return env\n","\n","    return thunk\n","\n","class NoisyLinear(nn.Module):\n","    def __init__(self, in_features, out_features, std_init=0.5):\n","        super().__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.std_init = std_init\n","\n","        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n","        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n","        self.register_buffer(\"weight_epsilon\", torch.FloatTensor(out_features, in_features))\n","        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n","        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n","        self.register_buffer(\"bias_epsilon\", torch.FloatTensor(out_features))\n","        # factorized gaussian noise\n","        self.reset_parameters()\n","        self.reset_noise()\n","\n","    def reset_parameters(self):\n","        mu_range = 1 / math.sqrt(self.in_features)\n","        self.weight_mu.data.uniform_(-mu_range, mu_range)\n","        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n","        self.bias_mu.data.uniform_(-mu_range, mu_range)\n","        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n","\n","    def reset_noise(self):\n","        self.weight_epsilon.normal_()\n","        self.bias_epsilon.normal_()\n","\n","    def forward(self, input):\n","        if self.training:\n","            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n","            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n","        else:\n","            weight = self.weight_mu\n","            bias = self.bias_mu\n","        return F.linear(input, weight, bias)\n","\n","\n","# ALGO LOGIC: initialize agent here:\n","class NoisyDuelingDistributionalNetwork(nn.Module):\n","    def __init__(self, env, n_atoms, v_min, v_max):\n","        super().__init__()\n","        self.n_atoms = n_atoms\n","        self.v_min = v_min\n","        self.v_max = v_max\n","        self.delta_z = (v_max - v_min) / (n_atoms - 1)\n","        self.n_actions = env.single_action_space.n\n","        self.register_buffer(\"support\", torch.linspace(v_min, v_max, n_atoms))\n","\n","        self.network = nn.Sequential(\n","            nn.Conv2d(4, 32, 8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, 4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, 3, stride=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","        )\n","        conv_output_size = 3136\n","\n","        self.value_head = nn.Sequential(NoisyLinear(conv_output_size, 512), nn.ReLU(), NoisyLinear(512, n_atoms))\n","\n","        self.advantage_head = nn.Sequential(\n","            NoisyLinear(conv_output_size, 512), nn.ReLU(), NoisyLinear(512, n_atoms * self.n_actions)\n","        )\n","\n","    def forward(self, x):\n","        h = self.network(x / 255.0)\n","        value = self.value_head(h).view(-1, 1, self.n_atoms)\n","        advantage = self.advantage_head(h).view(-1, self.n_actions, self.n_atoms)\n","        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n","        q_dist = F.softmax(q_atoms, dim=2)\n","        return q_dist\n","\n","    def reset_noise(self):\n","        for layer in self.value_head:\n","            if isinstance(layer, NoisyLinear):\n","                layer.reset_noise()\n","        for layer in self.advantage_head:\n","            if isinstance(layer, NoisyLinear):\n","                layer.reset_noise()\n","\n","\n","PrioritizedBatch = collections.namedtuple(\n","    \"PrioritizedBatch\", [\"observations\", \"actions\", \"rewards\", \"next_observations\", \"dones\", \"indices\", \"weights\"]\n",")\n","\n","\n","# adapted from: https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n","class SumSegmentTree:\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.tree_size = 2 * capacity - 1\n","        self.tree = np.zeros(self.tree_size, dtype=np.float32)\n","\n","    def _propagate(self, idx):\n","        parent = (idx - 1) // 2\n","        while parent >= 0:\n","            self.tree[parent] = self.tree[parent * 2 + 1] + self.tree[parent * 2 + 2]\n","            parent = (parent - 1) // 2\n","\n","    def update(self, idx, value):\n","        tree_idx = idx + self.capacity - 1\n","        self.tree[tree_idx] = value\n","        self._propagate(tree_idx)\n","\n","    def total(self):\n","        return self.tree[0]\n","\n","    def retrieve(self, value):\n","        idx = 0\n","        while idx * 2 + 1 < self.tree_size:\n","            left = idx * 2 + 1\n","            right = left + 1\n","            if value <= self.tree[left]:\n","                idx = left\n","            else:\n","                value -= self.tree[left]\n","                idx = right\n","        return idx - (self.capacity - 1)\n","\n","\n","# adapted from: https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n","class MinSegmentTree:\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.tree_size = 2 * capacity - 1\n","        self.tree = np.full(self.tree_size, float(\"inf\"), dtype=np.float32)\n","\n","    def _propagate(self, idx):\n","        parent = (idx - 1) // 2\n","        while parent >= 0:\n","            self.tree[parent] = min(self.tree[parent * 2 + 1], self.tree[parent * 2 + 2])\n","            parent = (parent - 1) // 2\n","\n","    def update(self, idx, value):\n","        tree_idx = idx + self.capacity - 1\n","        self.tree[tree_idx] = value\n","        self._propagate(tree_idx)\n","\n","    def min(self):\n","        return self.tree[0]\n","\n","\n","class PrioritizedReplayBuffer:\n","    def __init__(self, capacity, obs_shape, device, n_step, gamma, alpha=0.6, beta=0.4, eps=1e-6):\n","        self.capacity = capacity\n","        self.device = device\n","        self.n_step = n_step\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.eps = eps\n","\n","        self.buffer_obs = np.zeros((capacity,) + obs_shape, dtype=np.uint8)\n","        self.buffer_next_obs = np.zeros((capacity,) + obs_shape, dtype=np.uint8)\n","        self.buffer_actions = np.zeros(capacity, dtype=np.int64)\n","        self.buffer_rewards = np.zeros(capacity, dtype=np.float32)\n","        self.buffer_dones = np.zeros(capacity, dtype=np.bool_)\n","\n","        self.pos = 0\n","        self.size = 0\n","        self.max_priority = 1.0\n","\n","        self.sum_tree = SumSegmentTree(capacity)\n","        self.min_tree = MinSegmentTree(capacity)\n","\n","        # For n-step returns\n","        self.n_step_buffer = deque(maxlen=n_step)\n","\n","    def _get_n_step_info(self):\n","        reward = 0.0\n","        next_obs = self.n_step_buffer[-1][3]\n","        done = self.n_step_buffer[-1][4]\n","\n","        for i in range(len(self.n_step_buffer)):\n","            reward += self.gamma**i * self.n_step_buffer[i][2]\n","            if self.n_step_buffer[i][4]:\n","                next_obs = self.n_step_buffer[i][3]\n","                done = True\n","                break\n","        return reward, next_obs, done\n","\n","    def add(self, obs, action, reward, next_obs, done):\n","        self.n_step_buffer.append((obs, action, reward, next_obs, done))\n","\n","        if len(self.n_step_buffer) < self.n_step:\n","            return\n","\n","        reward, next_obs, done = self._get_n_step_info()\n","        obs = self.n_step_buffer[0][0]\n","        action = self.n_step_buffer[0][1]\n","\n","        idx = self.pos\n","        self.buffer_obs[idx] = obs\n","        self.buffer_next_obs[idx] = next_obs\n","        self.buffer_actions[idx] = action\n","        self.buffer_rewards[idx] = reward\n","        self.buffer_dones[idx] = done\n","\n","        priority = self.max_priority**self.alpha\n","        self.sum_tree.update(idx, priority)\n","        self.min_tree.update(idx, priority)\n","\n","        self.pos = (self.pos + 1) % self.capacity\n","        self.size = min(self.size + 1, self.capacity)\n","\n","        if done:\n","            self.n_step_buffer.clear()\n","\n","    def sample(self, batch_size):\n","        indices = []\n","        p_total = self.sum_tree.total()\n","\n","        # Safety check for total sum\n","        if p_total <= 0:\n","            p_total = 1.0\n","\n","        segment = p_total / batch_size\n","\n","        for i in range(batch_size):\n","            a = segment * i\n","            b = segment * (i + 1)\n","\n","            # Ensure valid range\n","            if b - a <= 0:\n","                b = a + 1e-6\n","\n","            # Clamp to valid bounds\n","            a = max(a, 0)\n","            b = min(b, p_total)\n","\n","            try:\n","                upperbound = np.random.uniform(a, b)\n","                idx = self.sum_tree.retrieve(upperbound)\n","\n","                # Ensure valid index\n","                idx = max(0, min(idx, self.size - 1))\n","                indices.append(idx)\n","\n","            except (OverflowError, ValueError):\n","                # Fallback to random sampling\n","                idx = np.random.randint(0, self.size)\n","                indices.append(idx)\n","\n","        samples = {\n","            \"observations\": torch.from_numpy(self.buffer_obs[indices]).to(self.device),\n","            \"actions\": torch.from_numpy(self.buffer_actions[indices]).to(self.device).unsqueeze(1),\n","            \"rewards\": torch.from_numpy(self.buffer_rewards[indices]).to(self.device).unsqueeze(1),\n","            \"next_observations\": torch.from_numpy(self.buffer_next_obs[indices]).to(self.device),\n","            \"dones\": torch.from_numpy(self.buffer_dones[indices]).to(self.device).unsqueeze(1),\n","        }\n","\n","        # Get priorities with safety checks\n","        probs = []\n","        for idx in indices:\n","            try:\n","                prob = self.sum_tree.tree[idx + self.capacity - 1]\n","                probs.append(max(prob, self.eps))  # Ensure no zeros\n","            except IndexError:\n","                probs.append(self.eps)\n","\n","        probs = np.array(probs, dtype=np.float64)\n","\n","        # Calculate weights with safety checks\n","        # Ensure probs are normalized and not zero\n","        probs = np.maximum(probs, self.eps)\n","        probs_normalized = probs / p_total\n","        probs_normalized = np.maximum(probs_normalized, 1e-8)\n","\n","        # Calculate importance sampling weights\n","        weights = np.power(self.size * probs_normalized, -self.beta)\n","\n","        # Handle potential overflow/underflow\n","        weights = np.clip(weights, 1e-8, 1e8)\n","\n","        # Normalize weights\n","        max_weight = np.max(weights)\n","        if max_weight > 0:\n","            weights = weights / max_weight\n","        else:\n","            weights = np.ones_like(weights)\n","\n","        samples[\"weights\"] = torch.from_numpy(weights).to(self.device).unsqueeze(1)\n","        samples[\"indices\"] = indices\n","\n","        return PrioritizedBatch(**samples)\n","\n","    def update_priorities(self, indices, priorities):\n","        # Ensure priorities are valid\n","        priorities = np.abs(priorities) + self.eps\n","        priorities = np.clip(priorities, self.eps, 1e6)  # Cap maximum priority\n","\n","        self.max_priority = max(self.max_priority, priorities.max())\n","\n","        for idx, priority in zip(indices, priorities):\n","            priority = priority**self.alpha\n","            self.sum_tree.update(idx, priority)\n","            self.min_tree.update(idx, priority)"]},{"cell_type":"code","execution_count":null,"id":"3HX4PMD60Jka","metadata":{"id":"3HX4PMD60Jka"},"outputs":[],"source":["def train_agent():\n","    episode_rewards = deque(maxlen=100)  # Initialize here\n","\n","\n","    @dataclass\n","    class Args:\n","        exp_name: str = script_name\n","        \"\"\"the name of this experiment\"\"\"\n","        seed: int = 1\n","        \"\"\"seed of the experiment\"\"\"\n","        torch_deterministic: bool = True\n","        \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n","        cuda: bool = True\n","        \"\"\"if toggled, cuda will be enabled by default\"\"\"\n","        track: bool = True\n","        \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n","        wandb_project_name: str = \"cleanRL\"\n","        \"\"\"the wandb's project name\"\"\"\n","        wandb_entity: str = None\n","        \"\"\"the entity (team) of wandb's project\"\"\"\n","        capture_video: bool = False\n","        \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n","        save_model: bool = True\n","        \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n","        upload_model: bool = False\n","        \"\"\"whether to upload the saved model to huggingface\"\"\"\n","        hf_entity: str = \"\"\n","        \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n","\n","        env_id: str = \"ALE/Pong-v5\"\n","        \"\"\"the id of the environment\"\"\"\n","        total_timesteps: int = 10000000 #10000000\n","        \"\"\"total timesteps of the experiments\"\"\"\n","        learning_rate: float = 1e-4 # 0.0000625\n","        \"\"\"the learning rate of the optimizer\"\"\"\n","        num_envs: int = 1\n","        \"\"\"the number of parallel game environments\"\"\"\n","        buffer_size: int = 500000 #1000000\n","        \"\"\"the replay memory buffer size\"\"\"\n","        gamma: float = 0.99\n","        \"\"\"the discount factor gamma\"\"\"\n","        tau: float = 1.0\n","        \"\"\"the target network update rate\"\"\"\n","        target_network_frequency: int = 8000\n","        \"\"\"the timesteps it takes to update the target network\"\"\"\n","        batch_size: int = 32\n","        \"\"\"the batch size of sample from the reply memory\"\"\"\n","        start_e: float = 0 # 1\n","        \"\"\"the starting epsilon for exploration\"\"\"\n","        end_e: float = 0 # 0.01\n","        \"\"\"the ending epsilon for exploration\"\"\"\n","        exploration_fraction: float = 0 # 0.1\n","        \"\"\"the fraction of `total-timesteps` it takes from start-e to go end-e\"\"\"\n","        learning_starts: int = 80000\n","        \"\"\"timestep to start learning\"\"\"\n","        train_frequency: int = 4\n","        \"\"\"the frequency of training\"\"\"\n","        n_step: int = 3\n","        \"\"\"the number of steps to look ahead for n-step Q learning\"\"\"\n","        prioritized_replay_alpha: float = 0.5\n","        \"\"\"alpha parameter for prioritized replay buffer\"\"\"\n","        prioritized_replay_beta: float = 0.4\n","        \"\"\"beta parameter for prioritized replay buffer\"\"\"\n","        prioritized_replay_eps: float = 1e-6\n","        \"\"\"epsilon parameter for prioritized replay buffer\"\"\"\n","        n_atoms: int = 51\n","        \"\"\"the number of atoms\"\"\"\n","        v_min: float = -10\n","        \"\"\"the return lower bound\"\"\"\n","        v_max: float = 10\n","        \"\"\"the return upper bound\"\"\"\n","\n","    import wandb\n","    wandb.init()\n","    config = wandb.config\n","\n","    args = Args()\n","\n","    for key in vars(args):\n","        if key in config:\n","            setattr(args, key, config[key])\n","\n","    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n","    if args.track:\n","        wandb.init(\n","            project=args.wandb_project_name,\n","            entity=args.wandb_entity,\n","            sync_tensorboard=True,\n","            config=vars(args),\n","            name=run_name,\n","            monitor_gym=True,\n","            save_code=True,\n","        )\n","    writer = SummaryWriter(f\"runs/{run_name}\")\n","    writer.add_text(\n","        \"hyperparameters\",\n","        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n","    )\n","\n","    # TRY NOT TO MODIFY: seeding\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    torch.backends.cudnn.deterministic = args.torch_deterministic\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n","\n","    # env setup\n","    envs = gym.vector.SyncVectorEnv(\n","        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n","    )\n","    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n","\n","    q_network = NoisyDuelingDistributionalNetwork(envs, args.n_atoms, args.v_min, args.v_max).to(device)\n","    optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate, eps=1.5e-4)\n","    target_network = NoisyDuelingDistributionalNetwork(envs, args.n_atoms, args.v_min, args.v_max).to(device)\n","    target_network.load_state_dict(q_network.state_dict())\n","\n","    rb = PrioritizedReplayBuffer(\n","        args.buffer_size,\n","        envs.single_observation_space.shape,\n","        device,\n","        args.n_step,\n","        args.gamma,\n","        args.prioritized_replay_alpha,\n","        args.prioritized_replay_beta,\n","        args.prioritized_replay_eps,\n","    )\n","\n","    start_time = time.time()\n","\n","    episodic_returns = np.zeros((envs.num_envs,))\n","    episodic_lengths = np.zeros((envs.num_envs,))\n","\n","    # TRY NOT TO MODIFY: start the game\n","    obs, _ = envs.reset(seed=args.seed)\n","    for global_step in range(args.total_timesteps):\n","        # anneal PER beta to 1\n","        rb.beta = min(\n","            1.0, args.prioritized_replay_beta + global_step * (1.0 - args.prioritized_replay_beta) / args.total_timesteps\n","        )\n","\n","        # ALGO LOGIC: put action logic here\n","        with torch.no_grad():\n","            q_dist = q_network(torch.Tensor(obs).to(device))\n","            q_values = torch.sum(q_dist * q_network.support, dim=2)\n","            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n","\n","        # TRY NOT TO MODIFY: execute the game and log data.\n","        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n","\n","        if terminations or truncations:\n","            # Log to TensorBoard\n","            print(f\"global_step={global_step}, episodic_return={episodic_returns}\")\n","            writer.add_scalar(\"charts/episodic_return\", episodic_returns, global_step)\n","            writer.add_scalar(\"charts/episodic_length\", episodic_lengths, global_step)\n","\n","            # Track rolling mean\n","            episode_rewards.append(episodic_returns)\n","            if len(episode_rewards) > 100:\n","                episode_rewards.pop(0)\n","\n","            mean_reward_100 = sum(episode_rewards) / len(episode_rewards)\n","\n","            writer.add_scalar(\"charts/mean_reward_100\", mean_reward_100, global_step)\n","\n","            if args.track:\n","                wandb.log({\n","                    \"charts/episodic_return\": episodic_returns,\n","                    \"charts/episodic_length\": episodic_lengths,\n","                    \"charts/mean_reward_100\": mean_reward_100,\n","                    \"global_step\": global_step\n","                })\n","\n","            # Reset tracking\n","            episodic_returns = 0\n","            episodic_lengths = 0\n","\n","\n","        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n","        real_next_obs = next_obs.copy()\n","        for idx, trunc in enumerate(truncations):\n","            if trunc:\n","                real_next_obs[idx] = infos[\"final_observation\"][idx]\n","        rb.add(obs, actions, rewards, real_next_obs, terminations)\n","\n","        # Accumulate rewards and lengths for each environment\n","        episodic_returns += rewards\n","        episodic_lengths += 1\n","\n","\n","        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n","        obs = next_obs\n","\n","        # ALGO LOGIC: training.\n","        if global_step > args.learning_starts:\n","            if global_step % args.train_frequency == 0:\n","                # reset the noise for both networks\n","                q_network.reset_noise()\n","                target_network.reset_noise()\n","                data = rb.sample(args.batch_size)\n","\n","                with torch.no_grad():\n","                    next_dist = target_network(data.next_observations)  # [B, num_actions, n_atoms]\n","                    support = target_network.support  # [n_atoms]\n","                    next_q_values = torch.sum(next_dist * support, dim=2)  # [B, num_actions]\n","\n","                    # double q-learning\n","                    next_dist_online = q_network(data.next_observations)  # [B, num_actions, n_atoms]\n","                    next_q_online = torch.sum(next_dist_online * support, dim=2)  # [B, num_actions]\n","                    best_actions = torch.argmax(next_q_online, dim=1)  # [B]\n","                    next_pmfs = next_dist[torch.arange(args.batch_size), best_actions]  # [B, n_atoms]\n","\n","                    # compute the n-step Bellman update.\n","                    gamma_n = args.gamma**args.n_step\n","                    next_atoms = data.rewards + gamma_n * support * (1 - data.dones.float())\n","                    tz = next_atoms.clamp(q_network.v_min, q_network.v_max)\n","\n","                    # projection\n","                    delta_z = q_network.delta_z\n","                    b = (tz - q_network.v_min) / delta_z  # shape: [B, n_atoms]\n","                    l = b.floor().clamp(0, args.n_atoms - 1)\n","                    u = b.ceil().clamp(0, args.n_atoms - 1)\n","\n","                    # (l == u).float() handles the case where bj is exactly an integer\n","                    # example bj = 1, then the upper ceiling should be uj= 2, and lj= 1\n","                    d_m_l = (u.float() + (l == b).float() - b) * next_pmfs  # [B, n_atoms]\n","                    d_m_u = (b - l) * next_pmfs  # [B, n_atoms]\n","\n","                    target_pmfs = torch.zeros_like(next_pmfs)\n","                    for i in range(target_pmfs.size(0)):\n","                        target_pmfs[i].index_add_(0, l[i].long(), d_m_l[i])\n","                        target_pmfs[i].index_add_(0, u[i].long(), d_m_u[i])\n","\n","                dist = q_network(data.observations)  # [B, num_actions, n_atoms]\n","                pred_dist = dist.gather(1, data.actions.unsqueeze(-1).expand(-1, -1, args.n_atoms)).squeeze(1)\n","                log_pred = torch.log(pred_dist.clamp(min=1e-5, max=1 - 1e-5))\n","\n","                loss_per_sample = -(target_pmfs * log_pred).sum(dim=1)\n","                loss = (loss_per_sample * data.weights.squeeze()).mean()\n","\n","                # update priorities\n","                new_priorities = loss_per_sample.detach().cpu().numpy()\n","                rb.update_priorities(data.indices, new_priorities)\n","\n","                if global_step % 100 == 0:\n","                    writer.add_scalar(\"losses/td_loss\", loss.item(), global_step)\n","                    q_values = (pred_dist * q_network.support).sum(dim=1)  # [B]\n","                    writer.add_scalar(\"losses/q_values\", q_values.mean().item(), global_step)\n","                    sps = int(global_step / (time.time() - start_time))\n","                    print(\"SPS:\", sps)\n","                    writer.add_scalar(\"charts/SPS\", sps, global_step)\n","                    writer.add_scalar(\"charts/beta\", rb.beta, global_step)\n","\n","                # optimize the model\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","            # update target network\n","            if global_step % args.target_network_frequency == 0:\n","                for target_param, param in zip(target_network.parameters(), q_network.parameters()):\n","                    target_param.data.copy_(args.tau * param.data + (1.0 - args.tau) * target_param.data)\n","\n","    envs.close()\n","    writer.close()\n","\n","    # Return the mean reward over last 100 episodes, or all episodes if less than 100\n","    if episode_rewards:\n","        final_mean_reward = sum(episode_rewards) / len(episode_rewards)\n","    else:\n","        final_mean_reward = 0\n","\n","    if args.track:\n","        wandb.log({\n","            \"mean_reward_100\": final_mean_reward,\n","            \"total_episodes\": len(episode_rewards)\n","        })\n","\n","    return final_mean_reward\n"]},{"cell_type":"code","execution_count":null,"id":"a1dvEhIbz502","metadata":{"id":"a1dvEhIbz502"},"outputs":[],"source":["# Define sweep configuration\n","sweep_config = {\n","    'method': 'bayes',\n","    'metric': {\n","        'name': 'mean_reward_100',\n","        'goal': 'maximize'\n","    },\n","    'parameters': {\n","        'learning_rate': {\n","            'distribution': 'log_uniform_values',\n","            'min': 1e-5,\n","            'max': 1e-2\n","        },\n","        'batch_size': {\n","            'values': [32, 64, 128]\n","        },\n","        'buffer_size': {\n","            'values': [50000, 100000, 250000, 500000]  # Smaller for Colab memory limits\n","        },\n","        'gamma': {\n","            'distribution': 'uniform',\n","            'min': 0.95,\n","            'max': 0.999\n","        },\n","        'tau': {\n","            'distribution': 'uniform',\n","            'min': 0.001,\n","            'max': 0.01\n","        },\n","        'n_step': {\n","            'values': [1, 3, 5]\n","        },\n","        'prioritized_replay_alpha': {\n","            'distribution': 'uniform',\n","            'min': 0.5,\n","            'max': 0.8\n","        },\n","        'prioritized_replay_beta': {\n","            'distribution': 'uniform',\n","            'min': 0.4,\n","            'max': 0.6\n","        },\n","        'target_network_frequency': {\n","            'values': [500, 1000, 2000]\n","        },\n","        'train_frequency': {\n","            'values': [1, 4, 8]\n","        },\n","        # Fixed parameters\n","        'env_id': {'value': 'ALE/Pong-v5'},  # Use simple env for Colab\n","        'total_timesteps': {'value': 500000},  # Reduced for faster training\n","        'learning_starts': {'value': 1000},\n","        'seed': {'value': 42},\n","        'n_atoms': {'value': 51},\n","        'v_min': {'value': -10},\n","        'v_max': {'value': 10},\n","        'prioritized_replay_eps': {'value': 1e-6},\n","    }\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"595f158e-2e73-408e-a923-242b40a68248","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1D2JPa7KTm8tPdtzOsNeou9O4MTbdoDCx"},"id":"595f158e-2e73-408e-a923-242b40a68248","outputId":"16686e77-7154-4ea0-cf64-ee1674dca3fb"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import wandb\n","\n","sweep_id = wandb.sweep(sweep_config, project=\"dqn-colab-sweep\")\n","wandb.agent(sweep_id, train_agent, count=40)  # Run 10 trials\n"]},{"cell_type":"code","execution_count":null,"id":"_Kgmh3DBRVsI","metadata":{"id":"_Kgmh3DBRVsI"},"outputs":[],"source":["%tb"]},{"cell_type":"code","source":[],"metadata":{"id":"VaeKKTD80b7r"},"id":"VaeKKTD80b7r","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":5}