{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c0ff8d-7fd6-4f71-9ec8-8283454cddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d22f66-0bea-4784-abab-5c1aef283b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from dqn_pong_best (3).pth copy 2\n",
      "Playing 1 games...\n",
      "Episode 1: Total Reward = -21.0\n",
      "Visualization complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from collections import deque\n",
    "import ale_py\n",
    "\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "MODEL_PATH = \"dqn_pong_best (2).pth copy 3\"  # Path to your trained model\n",
    "NUM_EPISODES = 1                  # Number of games to play\n",
    "RENDER_DELAY = 5                 # ms between frames (0 for fastest)\n",
    "# =========================\n",
    "\n",
    "# Image preprocessing (must match training parameters)\n",
    "def preprocess(obs,\n",
    "               crop_top=34,\n",
    "               crop_bottom=16,\n",
    "               crop_left=0,\n",
    "               crop_right=0,\n",
    "               resize_width=84,\n",
    "               resize_height=84):\n",
    "    if len(obs.shape) == 3 and obs.shape[2] == 3:\n",
    "        greyscaled = np.dot(obs[..., :3], [0.299, 0.587, 0.114])\n",
    "    else:\n",
    "        greyscaled = obs\n",
    "    height, width = greyscaled.shape\n",
    "    cropped = greyscaled[crop_top:height-crop_bottom, crop_left:width-crop_right]\n",
    "    resized = cv2.resize(cropped, (resize_width, resize_height), interpolation=cv2.INTER_NEAREST)\n",
    "    return resized\n",
    "\n",
    "# Neural Network (must match training architecture)\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, num_actions, in_channels=4):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = torch.nn.Linear(64 * 7 * 7, 512)  # Adjusted for 84x84 input\n",
    "        self.fc2 = torch.nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\")\n",
    "\n",
    "# Load trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN(env.action_space.n).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model from {MODEL_PATH}\")\n",
    "print(f\"Playing {NUM_EPISODES} games...\")\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    obs, _ = env.reset()\n",
    "    frame = preprocess(obs)\n",
    "    frame_stack = deque([frame] * 4, maxlen=4)\n",
    "    state = np.stack(frame_stack, axis=0)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Prepare state tensor\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) / 255.0\n",
    "        \n",
    "        # Get Q-values and select best action\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "        action = q_values.argmax().item()\n",
    "        \n",
    "        # Take action\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update frame stack\n",
    "        next_frame = preprocess(next_obs)\n",
    "        frame_stack.append(next_frame)\n",
    "        next_state = np.stack(frame_stack, axis=0)\n",
    "        state = next_state\n",
    "        \n",
    "        # Add small delay for visualization\n",
    "        if RENDER_DELAY > 0:\n",
    "            cv2.waitKey(RENDER_DELAY)\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Total Reward = {total_reward}\")\n",
    "\n",
    "env.close()\n",
    "print(\"Visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee6fa6-77a1-45ad-b07f-2642ee4bb03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debd7a2-ff90-4f3a-be72-9bbb240ec66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84380518-bdfa-4259-aa55-90a8093ed35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33213c2-7cc0-4391-a2f9-208806e0c29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1da255-f19f-4885-9210-4a944e4e4fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac405788-4949-4022-82a2-8b998711101a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b14ce-4b13-4f8e-bc6a-75c561abd6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
